{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_idsgame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, defaultdict\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Tutte le librerie sono state importate con successo!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device disponibile: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57495e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(scenario=\"random_attack\"):\n",
    "    if scenario == \"random_attack\":\n",
    "        env_name = \"idsgame-random_attack-v21\"\n",
    "    elif scenario == \"maximal_attack\":\n",
    "        env_name = \"idsgame-maximal_attack-v21\"\n",
    "    else:\n",
    "        raise ValueError(f\"Scenario non supportato: {scenario}\")\n",
    "    \n",
    "    try:\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nella creazione dell'ambiente: {e}\")\n",
    "        return gym.make('CartPole-v1')\n",
    "\n",
    "def explore_environment(env):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CARATTERISTICHE DELL'AMBIENTE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]\n",
    "    \n",
    "    print(f\"Spazio delle osservazioni: {env.observation_space}\")\n",
    "    print(f\"Spazio delle azioni: {env.action_space}\")\n",
    "    print(f\"Dimensione stato iniziale: {np.array(state).shape}\")\n",
    "    print(f\"Numero di azioni possibili: {env.action_space.n if hasattr(env.action_space, 'n') else 'Continuo'}\")\n",
    "    \n",
    "    print(\"\\nSimulazione di 5 step casuali:\")\n",
    "    for i in range(5):\n",
    "        action = env.action_space.sample()\n",
    "        result = env.step(action)\n",
    "        \n",
    "        if len(result) == 4:\n",
    "            next_state, reward, done, info = result\n",
    "        else:\n",
    "            next_state, reward, done, truncated, info = result\n",
    "            \n",
    "        print(f\"   Step {i+1}: Azione={action}, Reward={reward:.3f}, Done={done}\")\n",
    "        \n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "            print(\"   Ambiente resettato\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    return env\n",
    "\n",
    "env_random = create_environment(\"random_attack\")\n",
    "env_random = explore_environment(env_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57183a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, \n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.q_table = defaultdict(lambda: np.zeros(action_size))\n",
    "        \n",
    "        self.training_scores = []\n",
    "        self.training_epsilons = []\n",
    "        self.training_losses = []\n",
    "    \n",
    "    def _state_to_key(self, state):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            return tuple(np.round(state, decimals=2))\n",
    "        else:\n",
    "            return state\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        state_key = self._state_to_key(state)\n",
    "        \n",
    "        if training and np.random.random() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_key])\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state, next_action):\n",
    "        state_key = self._state_to_key(state)\n",
    "        next_state_key = self._state_to_key(next_state)\n",
    "        \n",
    "        current_q = self.q_table[state_key][action]\n",
    "        next_q = self.q_table[next_state_key][next_action]\n",
    "        \n",
    "        target = reward + self.discount_factor * next_q\n",
    "        td_error = target - current_q\n",
    "        \n",
    "        self.q_table[state_key][action] += self.learning_rate * td_error\n",
    "        \n",
    "        return abs(td_error)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def train(self, env, episodes=1000, max_steps=200, verbose=True):\n",
    "        print(\"Inizio addestramento SARSA...\")\n",
    "        print(f\"Episodi: {episodes}, Max steps per episodio: {max_steps}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "            \n",
    "            action = self.get_action(state, training=True)\n",
    "            \n",
    "            total_reward = 0\n",
    "            episode_loss = 0\n",
    "            steps = 0\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                result = env.step(action)\n",
    "                \n",
    "                if len(result) == 4:\n",
    "                    next_state, reward, done, info = result\n",
    "                else:\n",
    "                    next_state, reward, done, truncated, info = result\n",
    "                    done = done or truncated\n",
    "                \n",
    "                next_action = self.get_action(next_state, training=True)\n",
    "                \n",
    "                td_error = self.update_q_table(state, action, reward, next_state, next_action)\n",
    "                episode_loss += td_error\n",
    "                \n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            self.decay_epsilon()\n",
    "            \n",
    "            self.training_scores.append(total_reward)\n",
    "            self.training_epsilons.append(self.epsilon)\n",
    "            self.training_losses.append(episode_loss / max(steps, 1))\n",
    "            \n",
    "            if verbose and (episode + 1) % 100 == 0:\n",
    "                avg_score = np.mean(self.training_scores[-100:])\n",
    "                print(f\"Episodio {episode + 1:4d} | Score medio (100): {avg_score:8.2f} | \"\n",
    "                      f\"Epsilon: {self.epsilon:.3f} | Steps: {steps:3d}\")\n",
    "        \n",
    "        print(\"\\nAddestramento SARSA completato!\")\n",
    "        return self.training_scores, self.training_epsilons, self.training_losses\n",
    "\n",
    "print(\"Creazione dell'agente SARSA...\")\n",
    "\n",
    "SARSA_PARAMS = {\n",
    "    'learning_rate': 0.1,\n",
    "    'discount_factor': 0.95,\n",
    "    'epsilon': 1.0,\n",
    "    'epsilon_decay': 0.995,\n",
    "    'epsilon_min': 0.01\n",
    "}\n",
    "\n",
    "state_sample = env_random.reset()\n",
    "if isinstance(state_sample, tuple):\n",
    "    state_sample = state_sample[0]\n",
    "\n",
    "state_size = np.array(state_sample).size\n",
    "action_size = env_random.action_space.n if hasattr(env_random.action_space, 'n') else 4\n",
    "\n",
    "print(f\"Dimensioni - Stati: {state_size}, Azioni: {action_size}\")\n",
    "\n",
    "sarsa_agent = SARSAAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    **SARSA_PARAMS\n",
    ")\n",
    "\n",
    "print(\"Agente SARSA inizializzato con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avvio dell'addestramento SARSA per Random Attack scenario...\")\n",
    "\n",
    "TRAINING_EPISODES = 1500\n",
    "MAX_STEPS = 200\n",
    "\n",
    "start_time = time.time()\n",
    "sarsa_scores, sarsa_epsilons, sarsa_losses = sarsa_agent.train(\n",
    "    env_random, \n",
    "    episodes=TRAINING_EPISODES, \n",
    "    max_steps=MAX_STEPS,\n",
    "    verbose=True\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTempo di addestramento: {(end_time - start_time):.2f} secondi\")\n",
    "print(f\"Score finale medio (ultimi 100 episodi): {np.mean(sarsa_scores[-100:]):.2f}\")\n",
    "print(f\"Epsilon finale: {sarsa_agent.epsilon:.4f}\")\n",
    "print(f\"Miglioramento: {np.mean(sarsa_scores[-100:]) - np.mean(sarsa_scores[:100]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed72636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sarsa_results(scores, epsilons, losses):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Analisi Prestazioni SARSA - Random Attack Defense', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes[0, 0].plot(scores, alpha=0.6, color='steelblue', linewidth=0.8)\n",
    "    \n",
    "    window_size = 100\n",
    "    if len(scores) >= window_size:\n",
    "        moving_avg = pd.Series(scores).rolling(window=window_size).mean()\n",
    "        axes[0, 0].plot(moving_avg, color='red', linewidth=2, label=f'Media Mobile ({window_size})')\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    axes[0, 0].set_title('Score per Episodio')\n",
    "    axes[0, 0].set_xlabel('Episodio')\n",
    "    axes[0, 0].set_ylabel('Score Cumulativo')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(epsilons, color='orange', linewidth=2)\n",
    "    axes[0, 1].set_title('Decadimento Epsilon (Exploration → Exploitation)')\n",
    "    axes[0, 1].set_xlabel('Episodio')\n",
    "    axes[0, 1].set_ylabel('Epsilon')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].plot(losses, color='green', alpha=0.7, linewidth=1)\n",
    "    if len(losses) >= 50:\n",
    "        smooth_losses = pd.Series(losses).rolling(window=50).mean()\n",
    "        axes[1, 0].plot(smooth_losses, color='darkgreen', linewidth=2, label='Media Mobile (50)')\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    axes[1, 0].set_title('Errore Temporal Difference')\n",
    "    axes[1, 0].set_xlabel('Episodio')\n",
    "    axes[1, 0].set_ylabel('TD Error Medio')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    final_scores = scores[-200:] if len(scores) >= 200 else scores\n",
    "    axes[1, 1].hist(final_scores, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[1, 1].axvline(np.mean(final_scores), color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Media: {np.mean(final_scores):.2f}')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].set_title('Distribuzione Score Finali')\n",
    "    axes[1, 1].set_xlabel('Score')\n",
    "    axes[1, 1].set_ylabel('Frequenza')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sarsa_results(sarsa_scores, sarsa_epsilons, sarsa_losses)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALISI STATISTICA DETTAGLIATA - SARSA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Score medio totale: {np.mean(sarsa_scores):.3f}\")\n",
    "print(f\"Score medio ultimi 100 episodi: {np.mean(sarsa_scores[-100:]):.3f}\")\n",
    "print(f\"Score massimo raggiunto: {np.max(sarsa_scores):.3f}\")\n",
    "print(f\"Score minimo: {np.min(sarsa_scores):.3f}\")\n",
    "print(f\"Deviazione standard: {np.std(sarsa_scores):.3f}\")\n",
    "\n",
    "first_quarter = sarsa_scores[:len(sarsa_scores)//4]\n",
    "last_quarter = sarsa_scores[3*len(sarsa_scores)//4:]\n",
    "improvement = np.mean(last_quarter) - np.mean(first_quarter)\n",
    "\n",
    "print(f\"\\nMiglioramento primo → ultimo quarto: {improvement:.3f}\")\n",
    "print(f\"Riduzione esplorazione: {sarsa_epsilons[0]:.3f} → {sarsa_epsilons[-1]:.3f}\")\n",
    "\n",
    "stability_window = 200\n",
    "if len(sarsa_scores) >= stability_window:\n",
    "    recent_scores = sarsa_scores[-stability_window:]\n",
    "    coefficient_variation = np.std(recent_scores) / np.mean(recent_scores)\n",
    "    print(f\"Coefficiente di variazione (ultimi {stability_window}): {coefficient_variation:.3f}\")\n",
    "    \n",
    "    if coefficient_variation < 0.3:\n",
    "        print(\"Apprendimento STABILE\")\n",
    "    elif coefficient_variation < 0.5:\n",
    "        print(\"Apprendimento MODERATAMENTE STABILE\")\n",
    "    else:\n",
    "        print(\"Apprendimento INSTABILE\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_performance(agent, env, episodes=100, max_steps=200):\n",
    "    print(\"Inizio fase di testing dell'agente SARSA...\")\n",
    "    \n",
    "    test_scores = []\n",
    "    test_steps = []\n",
    "    successful_defenses = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.get_action(state, training=False)\n",
    "            \n",
    "            result = env.step(action)\n",
    "            if len(result) == 4:\n",
    "                next_state, reward, done, info = result\n",
    "            else:\n",
    "                next_state, reward, done, truncated, info = result\n",
    "                done = done or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        test_scores.append(total_reward)\n",
    "        test_steps.append(steps)\n",
    "        \n",
    "        if total_reward > 0:\n",
    "            successful_defenses += 1\n",
    "    \n",
    "    success_rate = (successful_defenses / episodes) * 100\n",
    "    avg_score = np.mean(test_scores)\n",
    "    avg_steps = np.mean(test_steps)\n",
    "    \n",
    "    print(f\"\\nRISULTATI DEL TEST (su {episodes} episodi):\")\n",
    "    print(f\"Score medio: {avg_score:.3f}\")\n",
    "    print(f\"Steps medi per episodio: {avg_steps:.1f}\")\n",
    "    print(f\"Tasso di successo difesa: {success_rate:.1f}%\")\n",
    "    print(f\"Score massimo: {np.max(test_scores):.3f}\")\n",
    "    print(f\"Score minimo: {np.min(test_scores):.3f}\")\n",
    "    \n",
    "    return test_scores, success_rate\n",
    "\n",
    "test_scores, success_rate = test_agent_performance(sarsa_agent, env_random, episodes=100)\n",
    "\n",
    "print(f\"\\nCONFRONTO TRAINING vs TEST:\")\n",
    "print(f\"Score medio training (ultimi 100): {np.mean(sarsa_scores[-100:]):.3f}\")\n",
    "print(f\"Score medio test: {np.mean(test_scores):.3f}\")\n",
    "print(f\"Differenza: {np.mean(test_scores) - np.mean(sarsa_scores[-100:]):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(test_scores, marker='o', alpha=0.7, linewidth=1, markersize=3)\n",
    "plt.axhline(np.mean(test_scores), color='red', linestyle='--', \n",
    "           label=f'Media: {np.mean(test_scores):.2f}')\n",
    "plt.title('Score durante il Testing')\n",
    "plt.xlabel('Episodio Test')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(test_scores, bins=15, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.axvline(np.mean(test_scores), color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Distribuzione Score Test')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Analisi Prestazioni Test - Agente SARSA', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cf245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_sizes=[256, 128, 64]):\n",
    "        super(DDQNNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        input_size = state_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(input_size, action_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.network(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor([e[0] for e in batch])\n",
    "        actions = torch.LongTensor([e[1] for e in batch])\n",
    "        rewards = torch.FloatTensor([e[2] for e in batch])\n",
    "        next_states = torch.FloatTensor([e[3] for e in batch])\n",
    "        dones = torch.BoolTensor([e[4] for e in batch])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DDQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, discount_factor=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, \n",
    "                 buffer_size=10000, batch_size=32, target_update_freq=100):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Usando device: {self.device}\")\n",
    "        \n",
    "        self.main_network = DDQNNetwork(state_size, action_size).to(self.device)\n",
    "        self.target_network = DDQNNetwork(state_size, action_size).to(self.device)\n",
    "        \n",
    "        self.update_target_network()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.main_network.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        self.training_scores = []\n",
    "        self.training_losses = []\n",
    "        self.training_epsilons = []\n",
    "        self.update_counter = 0\n",
    "    \n",
    "    def preprocess_state(self, state):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = state.flatten()\n",
    "        else:\n",
    "            state = np.array([state]).flatten()\n",
    "        \n",
    "        if np.max(state) > 1.0:\n",
    "            state = state / np.max(state)\n",
    "        \n",
    "        return torch.FloatTensor(state).to(self.device)\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        if training and np.random.random() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        \n",
    "        state_tensor = self.preprocess_state(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.main_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = self.preprocess_state(state).cpu().numpy()\n",
    "        next_state = self.preprocess_state(next_state).cpu().numpy()\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return 0\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        current_q_values = self.main_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_actions = self.main_network(next_states).argmax(1)\n",
    "            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            target_q_values = rewards + (self.discount_factor * next_q_values * (~dones))\n",
    "        \n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(self.main_network.parameters(), max_norm=1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "print(\"Architettura DDQN implementata con successo!\")\n",
    "print(f\"Reti neurali: {sum(p.numel() for p in DDQNNetwork(state_size, action_size).parameters())} parametri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cb51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddqn_agent(agent, env, episodes=2000, max_steps=200, verbose=True):\n",
    "    print(\"Inizio addestramento DDQN...\")\n",
    "    print(f\"Episodi: {episodes}, Max steps: {max_steps}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        \n",
    "        total_reward = 0\n",
    "        episode_loss = 0\n",
    "        loss_count = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.get_action(state, training=True)\n",
    "            \n",
    "            result = env.step(action)\n",
    "            if len(result) == 4:\n",
    "                next_state, reward, done, info = result\n",
    "            else:\n",
    "                next_state, reward, done, truncated, info = result\n",
    "                done = done or truncated\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            if len(agent.replay_buffer) >= agent.batch_size:\n",
    "                loss = agent.replay()\n",
    "                if loss > 0:\n",
    "                    episode_loss += loss\n",
    "                    loss_count += 1\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            agent.update_counter += 1\n",
    "            if agent.update_counter % agent.target_update_freq == 0:\n",
    "                agent.update_target_network()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        agent.training_scores.append(total_reward)\n",
    "        agent.training_epsilons.append(agent.epsilon)\n",
    "        agent.training_losses.append(episode_loss / max(loss_count, 1))\n",
    "        \n",
    "        if verbose and (episode + 1) % 200 == 0:\n",
    "            avg_score = np.mean(agent.training_scores[-100:])\n",
    "            avg_loss = np.mean(agent.training_losses[-100:])\n",
    "            print(f\"Episodio {episode + 1:4d} | Score medio: {avg_score:8.2f} | \"\n",
    "                  f\"Loss media: {avg_loss:.4f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    print(\"Addestramento DDQN completato!\")\n",
    "    return agent.training_scores, agent.training_losses, agent.training_epsilons\n",
    "\n",
    "print(\"Creazione agente DDQN per Random Attack...\")\n",
    "\n",
    "DDQN_PARAMS = {\n",
    "    'learning_rate': 0.001,\n",
    "    'discount_factor': 0.99,\n",
    "    'epsilon': 1.0,\n",
    "    'epsilon_decay': 0.995,\n",
    "    'epsilon_min': 0.01,\n",
    "    'buffer_size': 50000,\n",
    "    'batch_size': 32,\n",
    "    'target_update_freq': 100\n",
    "}\n",
    "\n",
    "print(\"Parametri DDQN configurati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correzione archittetura rete neurale...\")\n",
    "print(\"Rimozione BatchNorm per compatibilità con batch size = 1\")\n",
    "\n",
    "ddqn_agent_random = DDQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    **DDQN_PARAMS\n",
    ")\n",
    "\n",
    "print(\"Agente DDQN ricreato con architettura corretta!\")\n",
    "print(f\"Nuova architettura: {state_size} → 256 → 128 → 64 → {action_size} (senza BatchNorm)\")\n",
    "\n",
    "print(\"\\nTest rete neurale...\")\n",
    "test_state = env_random.reset()\n",
    "if isinstance(test_state, tuple):\n",
    "    test_state = test_state[0]\n",
    "\n",
    "try:\n",
    "    test_action = ddqn_agent_random.get_action(test_state, training=True)\n",
    "    print(f\"Test superato! Azione selezionata: {test_action}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore nel test: {e}\")\n",
    "    \n",
    "print(\"Pronto per l'addestramento!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f129b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avvio addestramento DDQN su Random Attack scenario...\")\n",
    "start_time = time.time()\n",
    "\n",
    "ddqn_scores_random, ddqn_losses_random, ddqn_epsilons_random = train_ddqn_agent(\n",
    "    ddqn_agent_random, \n",
    "    env_random, \n",
    "    episodes=1000,\n",
    "    max_steps=200,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTempo addestramento Random Attack: {(end_time - start_time):.2f} secondi\")\n",
    "print(f\"Score finale medio: {np.mean(ddqn_scores_random[-100:]):.2f}\")\n",
    "print(f\"Epsilon finale: {ddqn_agent_random.epsilon:.4f}\")\n",
    "print(f\"Miglioramento: {np.mean(ddqn_scores_random[-100:]) - np.mean(ddqn_scores_random[:100]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setup ambiente per Maximal Attack scenario...\")\n",
    "\n",
    "try:\n",
    "    env_maximal = create_environment(\"maximal_attack\")\n",
    "    print(\"Ambiente Maximal Attack creato\")\n",
    "except:\n",
    "    print(\"Usando ambiente di fallback per Maximal Attack\")\n",
    "    env_maximal = env_random\n",
    "\n",
    "print(\"\\nEsplorazione Maximal Attack environment:\")\n",
    "env_maximal = explore_environment(env_maximal)\n",
    "\n",
    "print(\"\\nCreazione agente DDQN per Maximal Attack...\")\n",
    "\n",
    "ddqn_agent_maximal = DDQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    **DDQN_PARAMS\n",
    ")\n",
    "\n",
    "print(\"Agente DDQN per Maximal Attack inizializzato!\")\n",
    "\n",
    "print(\"\\nAddestramento DDQN su Maximal Attack scenario...\")\n",
    "print(\"Scenario più complesso - richiede strategie difensive avanzate\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "ddqn_scores_maximal, ddqn_losses_maximal, ddqn_epsilons_maximal = train_ddqn_agent(\n",
    "    ddqn_agent_maximal, \n",
    "    env_maximal, \n",
    "    episodes=2500,\n",
    "    max_steps=200,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTempo addestramento Maximal Attack: {(end_time - start_time):.2f} secondi\")\n",
    "print(f\"Score finale medio: {np.mean(ddqn_scores_maximal[-100:]):.2f}\")\n",
    "print(f\"Epsilon finale: {ddqn_agent_maximal.epsilon:.4f}\")\n",
    "\n",
    "print(f\"\\nCONFRONTO PRESTAZIONI DDQN:\")\n",
    "print(f\"Random Attack - Score medio finale: {np.mean(ddqn_scores_random[-100:]):.3f}\")\n",
    "print(f\"Maximal Attack - Score medio finale: {np.mean(ddqn_scores_maximal[-100:]):.3f}\")\n",
    "\n",
    "improvement_random = np.mean(ddqn_scores_random[-100:]) - np.mean(ddqn_scores_random[:100])\n",
    "improvement_maximal = np.mean(ddqn_scores_maximal[-100:]) - np.mean(ddqn_scores_maximal[:100])\n",
    "\n",
    "print(f\"Miglioramento Random Attack: {improvement_random:.3f}\")\n",
    "print(f\"Miglioramento Maximal Attack: {improvement_maximal:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ddqn_comprehensive_results(scores_random, losses_random, epsilons_random,\n",
    "                                   scores_maximal, losses_maximal, epsilons_maximal):\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 15))\n",
    "    fig.suptitle('Analisi Completa DDQN - Random vs Maximal Attack Defense', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    color_random = 'steelblue'\n",
    "    color_maximal = 'darkred'\n",
    "    \n",
    "    axes[0, 0].plot(scores_random, alpha=0.6, color=color_random, linewidth=0.8, label='Random Attack')\n",
    "    axes[0, 1].plot(scores_maximal, alpha=0.6, color=color_maximal, linewidth=0.8, label='Maximal Attack')\n",
    "    \n",
    "    window = 100\n",
    "    if len(scores_random) >= window:\n",
    "        ma_random = pd.Series(scores_random).rolling(window=window).mean()\n",
    "        axes[0, 0].plot(ma_random, color='red', linewidth=2, label=f'Media Mobile ({window})')\n",
    "        \n",
    "    if len(scores_maximal) >= window:\n",
    "        ma_maximal = pd.Series(scores_maximal).rolling(window=window).mean()\n",
    "        axes[0, 1].plot(ma_maximal, color='orange', linewidth=2, label=f'Media Mobile ({window})')\n",
    "    \n",
    "    axes[0, 0].set_title('Score - Random Attack')\n",
    "    axes[0, 0].set_xlabel('Episodio')\n",
    "    axes[0, 0].set_ylabel('Score Cumulativo')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].set_title('Score - Maximal Attack')\n",
    "    axes[0, 1].set_xlabel('Episodio')\n",
    "    axes[0, 1].set_ylabel('Score Cumulativo')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].plot(losses_random, alpha=0.7, color=color_random, linewidth=1)\n",
    "    if len(losses_random) >= 50:\n",
    "        smooth_loss_random = pd.Series(losses_random).rolling(window=50).mean()\n",
    "        axes[1, 0].plot(smooth_loss_random, color='darkblue', linewidth=2, label='Smooth Loss')\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    axes[1, 1].plot(losses_maximal, alpha=0.7, color=color_maximal, linewidth=1)\n",
    "    if len(losses_maximal) >= 50:\n",
    "        smooth_loss_maximal = pd.Series(losses_maximal).rolling(window=50).mean()\n",
    "        axes[1, 1].plot(smooth_loss_maximal, color='darkred', linewidth=2, label='Smooth Loss')\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    axes[1, 0].set_title('Training Loss - Random Attack')\n",
    "    axes[1, 0].set_xlabel('Episodio')\n",
    "    axes[1, 0].set_ylabel('MSE Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].set_title('Training Loss - Maximal Attack')\n",
    "    axes[1, 1].set_xlabel('Episodio')\n",
    "    axes[1, 1].set_ylabel('MSE Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2, 0].plot(epsilons_random, color='orange', linewidth=2, label='Random Attack')\n",
    "    axes[2, 0].plot(epsilons_maximal, color='purple', linewidth=2, label='Maximal Attack')\n",
    "    axes[2, 0].set_title('Confronto Epsilon Decay')\n",
    "    axes[2, 0].set_xlabel('Episodio')\n",
    "    axes[2, 0].set_ylabel('Epsilon')\n",
    "    axes[2, 0].legend()\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    final_random = scores_random[-200:] if len(scores_random) >= 200 else scores_random\n",
    "    final_maximal = scores_maximal[-200:] if len(scores_maximal) >= 200 else scores_maximal\n",
    "    \n",
    "    axes[2, 1].hist(final_random, bins=15, alpha=0.6, color=color_random, \n",
    "                   label=f'Random (μ={np.mean(final_random):.2f})', density=True)\n",
    "    axes[2, 1].hist(final_maximal, bins=15, alpha=0.6, color=color_maximal, \n",
    "                   label=f'Maximal (μ={np.mean(final_maximal):.2f})', density=True)\n",
    "    axes[2, 1].set_title('Distribuzione Score Finali')\n",
    "    axes[2, 1].set_xlabel('Score')\n",
    "    axes[2, 1].set_ylabel('Densità')\n",
    "    axes[2, 1].legend()\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_ddqn_comprehensive_results(\n",
    "    ddqn_scores_random, ddqn_losses_random, ddqn_epsilons_random,\n",
    "    ddqn_scores_maximal, ddqn_losses_maximal, ddqn_epsilons_maximal\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALISI STATISTICA COMPLETA - DDQN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def analyze_performance(scores, scenario_name):\n",
    "    \"\"\"Analizza le prestazioni per uno scenario\"\"\"\n",
    "    print(f\"\\n{scenario_name.upper()}:\")\n",
    "    print(f\"   Score medio totale: {np.mean(scores):.3f}\")\n",
    "    print(f\"   Score medio ultimi 100: {np.mean(scores[-100:]):.3f}\")\n",
    "    print(f\"   Score massimo: {np.max(scores):.3f}\")\n",
    "    print(f\"   Score minimo: {np.min(scores):.3f}\")\n",
    "    print(f\"   Deviazione standard: {np.std(scores):.3f}\")\n",
    "    \n",
    "    first_quarter = scores[:len(scores)//4]\n",
    "    last_quarter = scores[3*len(scores)//4:]\n",
    "    improvement = np.mean(last_quarter) - np.mean(first_quarter)\n",
    "    print(f\"   Miglioramento: {improvement:.3f}\")\n",
    "    \n",
    "    if len(scores) >= 200:\n",
    "        recent = scores[-200:]\n",
    "        cv = np.std(recent) / np.mean(recent)\n",
    "        print(f\"   Coefficiente variazione: {cv:.3f}\")\n",
    "        \n",
    "        if cv < 0.3:\n",
    "            stability = \"STABILE\"\n",
    "        elif cv < 0.5:\n",
    "            stability = \"MODERATO\"\n",
    "        else:\n",
    "            stability = \"INSTABILE\"\n",
    "        print(f\"   Stabilità: {stability}\")\n",
    "\n",
    "analyze_performance(ddqn_scores_random, \"Random Attack\")\n",
    "analyze_performance(ddqn_scores_maximal, \"Maximal Attack\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cff87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ddqn_agents(agent_random, agent_maximal, env_random, env_maximal, episodes=100):\n",
    "    print(\"TEST FINALE AGENTI DDQN\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\nTesting agente Random Attack...\")\n",
    "    test_scores_random = []\n",
    "    for episode in range(episodes):\n",
    "        state = env_random.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        \n",
    "        total_reward = 0\n",
    "        for step in range(200):\n",
    "            action = agent_random.get_action(state, training=False)\n",
    "            result = env_random.step(action)\n",
    "            \n",
    "            if len(result) == 4:\n",
    "                next_state, reward, done, info = result\n",
    "            else:\n",
    "                next_state, reward, done, truncated, info = result\n",
    "                done = done or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        test_scores_random.append(total_reward)\n",
    "    \n",
    "    print(\"Testing agente Maximal Attack...\")\n",
    "    test_scores_maximal = []\n",
    "    for episode in range(episodes):\n",
    "        state = env_maximal.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        \n",
    "        total_reward = 0\n",
    "        for step in range(200):\n",
    "            action = agent_maximal.get_action(state, training=False)\n",
    "            result = env_maximal.step(action)\n",
    "            \n",
    "            if len(result) == 4:\n",
    "                next_state, reward, done, info = result\n",
    "            else:\n",
    "                next_state, reward, done, truncated, info = result\n",
    "                done = done or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        test_scores_maximal.append(total_reward)\n",
    "    \n",
    "    results['random'] = test_scores_random\n",
    "    results['maximal'] = test_scores_maximal\n",
    "    \n",
    "    print(f\"\\nRISULTATI TEST:\")\n",
    "    print(f\"Random Attack Defense - Score medio: {np.mean(test_scores_random):.3f}\")\n",
    "    print(f\"Maximal Attack Defense - Score medio: {np.mean(test_scores_maximal):.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "test_results = test_ddqn_agents(\n",
    "    ddqn_agent_random, ddqn_agent_maximal, \n",
    "    env_random, env_maximal, episodes=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_comparison():\n",
    "    print(\"ANALISI COMPARATIVA COMPLETA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    metrics = {\n",
    "        'SARSA (Random Attack)': {\n",
    "            'score_medio_finale': np.mean(sarsa_scores[-100:]),\n",
    "            'score_massimo': np.max(sarsa_scores),\n",
    "            'stabilita': np.std(sarsa_scores[-200:]) / np.mean(sarsa_scores[-200:]) if len(sarsa_scores) >= 200 else 0,\n",
    "            'convergenza': len(sarsa_scores),\n",
    "            'miglioramento': np.mean(sarsa_scores[-100:]) - np.mean(sarsa_scores[:100]),\n",
    "            'tasso_successo': len([s for s in test_scores if s > 0]) / len(test_scores) * 100\n",
    "        },\n",
    "        'DDQN (Random Attack)': {\n",
    "            'score_medio_finale': np.mean(ddqn_scores_random[-100:]),\n",
    "            'score_massimo': np.max(ddqn_scores_random),\n",
    "            'stabilita': np.std(ddqn_scores_random[-200:]) / np.mean(ddqn_scores_random[-200:]) if len(ddqn_scores_random) >= 200 else 0,\n",
    "            'convergenza': len(ddqn_scores_random),\n",
    "            'miglioramento': np.mean(ddqn_scores_random[-100:]) - np.mean(ddqn_scores_random[:100]),\n",
    "            'tasso_successo': len([s for s in test_results['random'] if s > 0]) / len(test_results['random']) * 100\n",
    "        },\n",
    "        'DDQN (Maximal Attack)': {\n",
    "            'score_medio_finale': np.mean(ddqn_scores_maximal[-100:]),\n",
    "            'score_massimo': np.max(ddqn_scores_maximal),\n",
    "            'stabilita': np.std(ddqn_scores_maximal[-200:]) / np.mean(ddqn_scores_maximal[-200:]) if len(ddqn_scores_maximal) >= 200 else 0,\n",
    "            'convergenza': len(ddqn_scores_maximal),\n",
    "            'miglioramento': np.mean(ddqn_scores_maximal[-100:]) - np.mean(ddqn_scores_maximal[:100]),\n",
    "            'tasso_successo': len([s for s in test_results['maximal'] if s > 0]) / len(test_results['maximal']) * 100\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    df_comparison = pd.DataFrame(metrics).T\n",
    "    df_comparison = df_comparison.round(3)\n",
    "    \n",
    "    print(\"\\nTABELLA COMPARATIVA PRESTAZIONI:\")\n",
    "    print(df_comparison.to_string())\n",
    "    \n",
    "    print(f\"\\nANALISI DETTAGLIATA:\")\n",
    "    print(f\"{'Metrica':<25} {'SARSA':<15} {'DDQN (Random)':<15} {'DDQN (Maximal)':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for metric in ['score_medio_finale', 'stabilita', 'miglioramento', 'tasso_successo']:\n",
    "        sarsa_val = metrics['SARSA (Random Attack)'][metric]\n",
    "        ddqn_random_val = metrics['DDQN (Random Attack)'][metric]\n",
    "        ddqn_maximal_val = metrics['DDQN (Maximal Attack)'][metric]\n",
    "        \n",
    "        print(f\"{metric:<25} {sarsa_val:<15.3f} {ddqn_random_val:<15.3f} {ddqn_maximal_val:<15.3f}\")\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "comparison_df = comprehensive_comparison()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Confronto Finale: SARSA vs DDQN per Sicurezza Informatica', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "scores_comparison = [\n",
    "    np.mean(sarsa_scores[-100:]),\n",
    "    np.mean(ddqn_scores_random[-100:]),\n",
    "    np.mean(ddqn_scores_maximal[-100:])\n",
    "]\n",
    "labels = ['SARSA\\n(Random)', 'DDQN\\n(Random)', 'DDQN\\n(Maximal)']\n",
    "colors = ['steelblue', 'darkgreen', 'darkred']\n",
    "\n",
    "bars = axes[0, 0].bar(labels, scores_comparison, color=colors, alpha=0.7)\n",
    "axes[0, 0].set_title('Score Medio Finale')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, score in zip(bars, scores_comparison):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "success_rates = [\n",
    "    len([s for s in test_scores if s > 0]) / len(test_scores) * 100,\n",
    "    len([s for s in test_results['random'] if s > 0]) / len(test_results['random']) * 100,\n",
    "    len([s for s in test_results['maximal'] if s > 0]) / len(test_results['maximal']) * 100\n",
    "]\n",
    "\n",
    "bars2 = axes[0, 1].bar(labels, success_rates, color=colors, alpha=0.7)\n",
    "axes[0, 1].set_title('Tasso di Successo Difesa (%)')\n",
    "axes[0, 1].set_ylabel('Percentuale')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, rate in zip(bars2, success_rates):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                   f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "window = 100\n",
    "sarsa_smooth = pd.Series(sarsa_scores).rolling(window=window).mean()\n",
    "ddqn_random_smooth = pd.Series(ddqn_scores_random).rolling(window=window).mean()\n",
    "ddqn_maximal_smooth = pd.Series(ddqn_scores_maximal).rolling(window=window).mean()\n",
    "\n",
    "axes[1, 0].plot(sarsa_smooth, color='steelblue', linewidth=2, label='SARSA (Random)')\n",
    "axes[1, 0].plot(ddqn_random_smooth, color='darkgreen', linewidth=2, label='DDQN (Random)')\n",
    "axes[1, 0].plot(ddqn_maximal_smooth, color='darkred', linewidth=2, label='DDQN (Maximal)')\n",
    "\n",
    "axes[1, 0].set_title('Learning Curves Comparazione')\n",
    "axes[1, 0].set_xlabel('Episodi')\n",
    "axes[1, 0].set_ylabel('Score (Media Mobile)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "test_data = [test_scores, test_results['random'], test_results['maximal']]\n",
    "axes[1, 1].boxplot(test_data, labels=labels)\n",
    "axes[1, 1].set_title('Distribuzione Score Test')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRACCOMANDAZIONI:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_score = max(scores_comparison)\n",
    "best_success = max(success_rates)\n",
    "\n",
    "if best_score == scores_comparison[0]:\n",
    "    best_algo_score = \"SARSA\"\n",
    "elif best_score == scores_comparison[1]:\n",
    "    best_algo_score = \"DDQN (Random Attack)\"\n",
    "else:\n",
    "    best_algo_score = \"DDQN (Maximal Attack)\"\n",
    "\n",
    "if best_success == success_rates[0]:\n",
    "    best_algo_success = \"SARSA\"\n",
    "elif best_success == success_rates[1]:\n",
    "    best_algo_success = \"DDQN (Random Attack)\"\n",
    "else:\n",
    "    best_algo_success = \"DDQN (Maximal Attack)\"\n",
    "\n",
    "print(f\"Migliore per score finale: {best_algo_score} ({best_score:.3f})\")\n",
    "print(f\"Migliore per tasso successo: {best_algo_success} ({best_success:.1f}%)\")\n",
    "print(f\"Più veloce convergenza: SARSA (episodi di training ridotti)\")\n",
    "print(f\"Più complesso ma versatile: DDQN (gestisce meglio scenari complessi)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
