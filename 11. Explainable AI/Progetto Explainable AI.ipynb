{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo utilizzato: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple, Dict, Any\n",
    "import time\n",
    "\n",
    "# Verifica disponibilità GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo utilizzato: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memoria GPU disponibile: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricato: 60000 train, 10000 test\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\"Classe per gestire il caricamento e preprocessing dei dati MNIST\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 128, test_batch_size: int = 64, subset_ratio: float = 0.2):\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.subset_ratio = subset_ratio\n",
    "        \n",
    "        # Trasformazioni per compatibilità con DenseNet (richiede 3 canali)\n",
    "        # Utilizziamo 112x112 per un buon bilanciamento tra qualità e prestazioni\n",
    "        self.transform_train = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((112, 112)),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet stats\n",
    "        ])\n",
    "        \n",
    "        self.transform_test = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((112, 112)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self._load_data()\n",
    "    \n",
    "    def _load_data(self):\n",
    "        \"\"\"Carica i dataset MNIST\"\"\"\n",
    "        self.trainset = torchvision.datasets.MNIST(\n",
    "            root='./data', train=True, download=True, transform=self.transform_train\n",
    "        )\n",
    "        self.testset = torchvision.datasets.MNIST(\n",
    "            root='./data', train=False, download=True, transform=self.transform_test\n",
    "        )\n",
    "        \n",
    "        # Utilizza un subset per accelerare il prototipaggio\n",
    "        if self.subset_ratio < 1.0:\n",
    "            # Subset training bilanciato\n",
    "            train_size = int(len(self.trainset) * self.subset_ratio)\n",
    "            train_indices = np.random.choice(len(self.trainset), train_size, replace=False)\n",
    "            self.trainset = Subset(self.trainset, train_indices)\n",
    "            \n",
    "            # Subset test proporzionale\n",
    "            test_size = int(len(self.testset) * (self.subset_ratio * 2))  # Più campioni per test affidabile\n",
    "            test_indices = np.random.choice(len(self.testset), test_size, replace=False)\n",
    "            self.testset = Subset(self.testset, test_indices)\n",
    "        \n",
    "        # DataLoader\n",
    "        self.trainloader = DataLoader(\n",
    "            self.trainset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        self.testloader = DataLoader(\n",
    "            self.testset, \n",
    "            batch_size=self.test_batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=0,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset caricato: {len(self.trainset):,} train, {len(self.testset):,} test\")\n",
    "    \n",
    "    def get_sample_batch(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Restituisce un batch di esempio per testing\"\"\"\n",
    "        return next(iter(self.testloader))\n",
    "\n",
    "# Inizializza il data manager\n",
    "data_manager = DataManager()\n",
    "trainloader = data_manager.trainloader\n",
    "testloader = data_manager.testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\frogora\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\frogora\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello creato con 1024 -> 10 neuroni nel classificatore\n",
      "=== ACCURACY PRIMA DEL FINE-TUNING ===\n"
     ]
    }
   ],
   "source": [
    "class MNISTClassifier:\n",
    "    \"\"\"Classe per il classificatore MNIST basato su DenseNet121 pre-addestrato\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"Costruisce il modello con DenseNet121 pre-addestrato\"\"\"\n",
    "        self.model = models.densenet121(pretrained=True)\n",
    "        \n",
    "        # Congela tutti i layer tranne l'ultimo\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Sostituisce il classificatore per MNIST (10 classi)\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(in_features, self.num_classes)\n",
    "        \n",
    "        # Solo l'ultimo layer è trainable\n",
    "        for param in self.model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        self.model = self.model.to(self.device)\n",
    "        print(f\"Modello creato con {in_features} -> {self.num_classes} neuroni nel classificatore\")\n",
    "        \n",
    "    def fine_tune(self, trainloader: DataLoader, epochs: int = 2, lr: float = 0.001):\n",
    "        \"\"\"Fine-tuning dell'ultimo layer\"\"\"\n",
    "        print(f\"Inizio fine-tuning per {epochs} epoche...\")\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.classifier.parameters(), lr=lr)\n",
    "        \n",
    "        # Abilita mixed precision se GPU disponibile\n",
    "        use_amp = torch.cuda.is_available()\n",
    "        if use_amp:\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for i, (inputs, labels) in enumerate(trainloader):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass con mixed precision se disponibile\n",
    "                if use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                if i % 100 == 99:  # Progress ogni 100 batch\n",
    "                    print(f'[Epoca {epoch+1}, Batch {i+1}] Loss: {running_loss/100:.3f}, '\n",
    "                          f'Accuracy: {100*correct/total:.2f}%')\n",
    "                    running_loss = 0.0\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            epoch_acc = 100 * correct / total\n",
    "            print(f'Epoca {epoch+1} completata in {epoch_time:.1f}s - Accuracy: {epoch_acc:.2f}%')\n",
    "        \n",
    "        print(\"Fine-tuning completato!\")\n",
    "        \n",
    "    def evaluate(self, testloader: DataLoader) -> float:\n",
    "        \"\"\"Valuta l'accuracy del modello sul test set\"\"\"\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Mixed precision anche per inference se disponibile\n",
    "                if torch.cuda.is_available():\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(inputs)\n",
    "                else:\n",
    "                    outputs = self.model(inputs)\n",
    "                    \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Accuracy sul test set: {accuracy:.2f}% ({correct}/{total})')\n",
    "        return accuracy\n",
    "    \n",
    "    def get_model(self):\n",
    "        \"\"\"Restituisce il modello PyTorch\"\"\"\n",
    "        return self.model\n",
    "\n",
    "# Crea e addestra il classificatore\n",
    "classifier = MNISTClassifier()\n",
    "\n",
    "# Prima verifichiamo l'accuracy senza fine-tuning\n",
    "print(\"=== ACCURACY PRIMA DEL FINE-TUNING ===\")\n",
    "accuracy_before = classifier.evaluate(testloader)\n",
    "\n",
    "# Esegui fine-tuning\n",
    "print(\"\\n=== FINE-TUNING ===\")\n",
    "classifier.fine_tune(trainloader, epochs=2)\n",
    "\n",
    "# Verifica accuracy dopo fine-tuning\n",
    "print(\"\\n=== ACCURACY DOPO IL FINE-TUNING ===\")\n",
    "accuracy_after = classifier.evaluate(testloader)\n",
    "\n",
    "print(f\"\\nMiglioramento: {accuracy_before:.2f}% -> {accuracy_after:.2f}% (+{accuracy_after-accuracy_before:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install captum\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import LayerGradCam, LayerAttribution, Occlusion\n",
    "import numpy as np\n",
    "\n",
    "class ExplainabilityAnalyzer:\n",
    "    \"\"\"Classe per l'analisi di explainability dei modelli\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Identifica il target layer per GradCAM (ultimo layer convoluzionale)\n",
    "        self.target_layer = self._find_target_layer()\n",
    "        \n",
    "        # Inizializza gli explainer\n",
    "        self.gradcam = LayerGradCam(self.model, self.target_layer)\n",
    "        self.occlusion = Occlusion(self.model)\n",
    "        \n",
    "    def _find_target_layer(self):\n",
    "        \"\"\"Trova l'ultimo layer convoluzionale per GradCAM\"\"\"\n",
    "        # Per DenseNet121, l'ultimo layer convoluzionale è features[-1]\n",
    "        return self.model.features[-1]\n",
    "    \n",
    "    def explain_with_gradcam(self, input_tensor: torch.Tensor, target_class: int = None) -> torch.Tensor:\n",
    "        \"\"\"Genera spiegazione con GradCAM\"\"\"\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        \n",
    "        if target_class is None:\n",
    "            # Se non specificato, usa la predizione del modello\n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_tensor)\n",
    "                target_class = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        # Genera attributions\n",
    "        attributions = self.gradcam.attribute(input_tensor, target=target_class)\n",
    "        \n",
    "        # Upsample alle dimensioni dell'input\n",
    "        upsampled_attr = LayerAttribution.interpolate(\n",
    "            attributions, input_tensor.shape[2:]\n",
    "        )\n",
    "        \n",
    "        return upsampled_attr, target_class\n",
    "    \n",
    "    def explain_with_occlusion(self, input_tensor: torch.Tensor, target_class: int = None,\n",
    "                              sliding_window_shapes: Tuple = (3, 15, 15),\n",
    "                              strides: Tuple = (3, 8, 8)) -> torch.Tensor:\n",
    "        \"\"\"Genera spiegazione con Occlusion\"\"\"\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        \n",
    "        if target_class is None:\n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_tensor)\n",
    "                target_class = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        attributions = self.occlusion.attribute(\n",
    "            input_tensor,\n",
    "            strides=strides,\n",
    "            target=target_class,\n",
    "            sliding_window_shapes=sliding_window_shapes\n",
    "        )\n",
    "        \n",
    "        return attributions, target_class\n",
    "    \n",
    "    def visualize_explanation(self, input_tensor: torch.Tensor, attributions: torch.Tensor,\n",
    "                            predicted_class: int, true_class: int = None, method_name: str = \"\"):\n",
    "        \"\"\"Visualizza l'immagine originale e la saliency map\"\"\"\n",
    "        \n",
    "        # Prepara l'immagine per la visualizzazione\n",
    "        img = input_tensor.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
    "        \n",
    "        # Denormalizza l'immagine (ImageNet normalization)\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Prepara la saliency map\n",
    "        saliency = attributions.squeeze().detach().cpu().numpy()\n",
    "        if saliency.ndim == 3:\n",
    "            saliency = saliency.mean(axis=0)\n",
    "        \n",
    "        # Normalizza la saliency map\n",
    "        saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
    "        \n",
    "        # Crea la visualizzazione\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Immagine originale\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title(\"Immagine Originale\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Saliency map\n",
    "        im = axes[1].imshow(saliency, cmap='hot')\n",
    "        axes[1].set_title(f\"Saliency Map - {method_name}\")\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[1])\n",
    "        \n",
    "        # Overlay\n",
    "        axes[2].imshow(img, alpha=0.6)\n",
    "        axes[2].imshow(saliency, cmap='hot', alpha=0.4)\n",
    "        title = f\"Overlay - Predetto: {predicted_class}\"\n",
    "        if true_class is not None:\n",
    "            title += f\", Vero: {true_class}\"\n",
    "        axes[2].set_title(title)\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def comprehensive_analysis(self, input_tensor: torch.Tensor, true_class: int = None):\n",
    "        \"\"\"Esegue un'analisi completa con tutti i metodi disponibili\"\"\"\n",
    "        \n",
    "        # Ottieni predizione\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor.to(self.device))\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_class = torch.argmax(output, dim=1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()\n",
    "        \n",
    "        print(f\"Predizione: Classe {predicted_class} (Confidenza: {confidence:.3f})\")\n",
    "        if true_class is not None:\n",
    "            print(f\"Classe vera: {true_class}\")\n",
    "            print(f\"Predizione {'CORRETTA' if predicted_class == true_class else 'ERRATA'}\")\n",
    "        \n",
    "        # GradCAM\n",
    "        print(\"\\n--- Analisi GradCAM ---\")\n",
    "        gradcam_attr, _ = self.explain_with_gradcam(input_tensor, predicted_class)\n",
    "        self.visualize_explanation(input_tensor, gradcam_attr, predicted_class, true_class, \"GradCAM\")\n",
    "        \n",
    "        # Occlusion\n",
    "        print(\"\\n--- Analisi Occlusion ---\")\n",
    "        occlusion_attr, _ = self.explain_with_occlusion(input_tensor, predicted_class)\n",
    "        self.visualize_explanation(input_tensor, occlusion_attr, predicted_class, true_class, \"Occlusion\")\n",
    "        \n",
    "        return {\n",
    "            'predicted_class': predicted_class,\n",
    "            'confidence': confidence,\n",
    "            'gradcam_attributions': gradcam_attr,\n",
    "            'occlusion_attributions': occlusion_attr\n",
    "        }\n",
    "\n",
    "# Inizializza l'analyzer\n",
    "model = classifier.get_model()\n",
    "explainer = ExplainabilityAnalyzer(model, device)\n",
    "\n",
    "print(\"Explainability Analyzer inizializzato con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Downgrade numpy\n",
    "#!pip install numpy==1.23.5 --force-reinstall\n",
    "\n",
    "# 2. Reinstalla lime\n",
    "#!pip install lime --force-reinstall\n",
    "\n",
    "try:\n",
    "    import lime\n",
    "    from lime import lime_image\n",
    "    from skimage.segmentation import mark_boundaries\n",
    "    LIME_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"LIME non disponibile. Installazione...\")\n",
    "    # %pip install lime scikit-image\n",
    "    # Poi riavviare il kernel\n",
    "    LIME_AVAILABLE = False\n",
    "\n",
    "class LIMEAnalyzer:\n",
    "    \"\"\"Classe per l'analisi LIME delle predizioni\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "        if LIME_AVAILABLE:\n",
    "            self.explainer = lime_image.LimeImageExplainer()\n",
    "        else:\n",
    "            print(\"LIME non disponibile. Installare con: pip install lime scikit-image\")\n",
    "    \n",
    "    def _batch_predict(self, images):\n",
    "        \"\"\"Funzione per predizioni batch richiesta da LIME\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Converte le immagini in tensori\n",
    "        batch_tensors = []\n",
    "        for img in images:\n",
    "            # LIME passa immagini RGB [0,1], dobbiamo normalizzarle per ImageNet\n",
    "            img_tensor = torch.tensor(img).permute(2, 0, 1).float()\n",
    "            \n",
    "            # Applica la normalizzazione ImageNet\n",
    "            normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            img_tensor = normalize(img_tensor)\n",
    "            batch_tensors.append(img_tensor)\n",
    "        \n",
    "        batch = torch.stack(batch_tensors).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(batch)\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        \n",
    "        return probs.detach().cpu().numpy()\n",
    "    \n",
    "    def explain_instance(self, input_tensor: torch.Tensor, num_features: int = 5,\n",
    "                        num_samples: int = 1000) -> Dict[str, Any]:\n",
    "        \"\"\"Genera spiegazione LIME per un'istanza\"\"\"\n",
    "        \n",
    "        if not LIME_AVAILABLE:\n",
    "            print(\"LIME non disponibile.\")\n",
    "            return {}\n",
    "        \n",
    "        # Prepara l'immagine per LIME (RGB [0,1])\n",
    "        img = input_tensor.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
    "        \n",
    "        # Denormalizza l'immagine\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Ottieni la predizione\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor.to(self.device))\n",
    "            predicted_class = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        # Genera spiegazione LIME\n",
    "        explanation = self.explainer.explain_instance(\n",
    "            img,\n",
    "            self._batch_predict,\n",
    "            top_labels=1,\n",
    "            hide_color=0,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "        \n",
    "        # Estrai immagine e mask\n",
    "        temp, mask = explanation.get_image_and_mask(\n",
    "            predicted_class,\n",
    "            positive_only=True,\n",
    "            num_features=num_features,\n",
    "            hide_rest=False\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'explanation': explanation,\n",
    "            'image': temp,\n",
    "            'mask': mask,\n",
    "            'predicted_class': predicted_class,\n",
    "            'original_image': img\n",
    "        }\n",
    "    \n",
    "    def visualize_lime_explanation(self, lime_result: Dict[str, Any]):\n",
    "        \"\"\"Visualizza i risultati LIME\"\"\"\n",
    "        \n",
    "        if not lime_result:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Immagine originale\n",
    "        axes[0].imshow(lime_result['original_image'])\n",
    "        axes[0].set_title(\"Immagine Originale\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Maschera LIME\n",
    "        axes[1].imshow(lime_result['mask'], cmap='gray')\n",
    "        axes[1].set_title(\"Maschera LIME\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Overlay con boundaries\n",
    "        overlay = mark_boundaries(lime_result['image'], lime_result['mask'])\n",
    "        axes[2].imshow(overlay)\n",
    "        axes[2].set_title(f\"LIME - Classe Predetta: {lime_result['predicted_class']}\")\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Inizializza LIME analyzer se disponibile\n",
    "if LIME_AVAILABLE:\n",
    "    lime_analyzer = LIMEAnalyzer(model, device)\n",
    "    print(\"LIME Analyzer inizializzato con successo!\")\n",
    "else:\n",
    "    print(\"LIME Analyzer non disponibile - installare le dipendenze necessarie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Occlusion\n",
    "\n",
    "class TrustworthinessEvaluator:\n",
    "    \"\"\"Classe principale per la valutazione di trustworthiness del modello\"\"\"\n",
    "    \n",
    "    def __init__(self, classifier: MNISTClassifier, data_manager: DataManager):\n",
    "        self.classifier = classifier\n",
    "        self.data_manager = data_manager\n",
    "        self.model = classifier.get_model()\n",
    "        self.device = device\n",
    "        \n",
    "        # Inizializza gli analyzer\n",
    "        self.explainer = ExplainabilityAnalyzer(self.model, self.device)\n",
    "        if LIME_AVAILABLE:\n",
    "            self.lime_analyzer = LIMEAnalyzer(self.model, self.device)\n",
    "        \n",
    "    def evaluate_sample_predictions(self, num_samples: int = 10):\n",
    "        \"\"\"Valuta predizioni su un campione casuale\"\"\"\n",
    "        print(f\"=== VALUTAZIONE SU {num_samples} CAMPIONI CASUALI ===\")\n",
    "        \n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        confidence_scores = []\n",
    "        \n",
    "        sample_loader = DataLoader(\n",
    "            self.data_manager.testset, \n",
    "            batch_size=1, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        for i, (image, true_label) in enumerate(sample_loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            image, true_label = image.to(self.device), true_label.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model(image)\n",
    "                probabilities = torch.softmax(output, dim=1)\n",
    "                predicted_class = torch.argmax(output, dim=1).item()\n",
    "                confidence = probabilities[0, predicted_class].item()\n",
    "            \n",
    "            is_correct = predicted_class == true_label.item()\n",
    "            correct_predictions += is_correct\n",
    "            total_samples += 1\n",
    "            confidence_scores.append(confidence)\n",
    "            \n",
    "            status = \"✓\" if is_correct else \"✗\"\n",
    "            print(f\"Campione {i+1}: {status} Predetto: {predicted_class}, \"\n",
    "                  f\"Vero: {true_label.item()}, Confidenza: {confidence:.3f}\")\n",
    "        \n",
    "        accuracy = correct_predictions / total_samples\n",
    "        avg_confidence = np.mean(confidence_scores)\n",
    "        \n",
    "        print(f\"\\nRisultati:\")\n",
    "        print(f\"Accuracy: {accuracy:.3f} ({correct_predictions}/{total_samples})\")\n",
    "        print(f\"Confidenza media: {avg_confidence:.3f}\")\n",
    "        \n",
    "        return accuracy, avg_confidence\n",
    "    \n",
    "    def analyze_misclassified_example(self):\n",
    "        \"\"\"Trova e analizza un esempio mal classificato\"\"\"\n",
    "        print(\"=== RICERCA ESEMPIO MAL CLASSIFICATO ===\")\n",
    "        \n",
    "        for images, true_labels in self.data_manager.testloader:\n",
    "            images, true_labels = images.to(self.device), true_labels.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(images)\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Trova il primo esempio mal classificato nel batch\n",
    "            for i in range(len(images)):\n",
    "                if predicted_classes[i] != true_labels[i]:\n",
    "                    # Estrai la singola immagine e label\n",
    "                    image = images[i:i+1]  # Mantieni la dimensione batch\n",
    "                    true_label = true_labels[i]\n",
    "                    predicted_class = predicted_classes[i].item()\n",
    "                    \n",
    "                    print(f\"Trovato esempio mal classificato:\")\n",
    "                    print(f\"Classe vera: {true_label.item()}, Predetta: {predicted_class}\")\n",
    "                    \n",
    "                    # Analisi completa dell'esempio\n",
    "                    print(\"\\n=== ANALISI EXPLAINABILITY ===\")\n",
    "                    results = self.explainer.comprehensive_analysis(image, true_label.item())\n",
    "                    \n",
    "                    # Analisi LIME se disponibile\n",
    "                    if LIME_AVAILABLE:\n",
    "                        print(\"\\n=== ANALISI LIME ===\")\n",
    "                        lime_results = self.lime_analyzer.explain_instance(image)\n",
    "                        self.lime_analyzer.visualize_lime_explanation(lime_results)\n",
    "                    \n",
    "                    return image, true_label.item(), predicted_class, results\n",
    "        \n",
    "        print(\"Nessun esempio mal classificato trovato nei primi batch!\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    def analyze_correct_example(self):\n",
    "        \"\"\"Trova e analizza un esempio correttamente classificato\"\"\"\n",
    "        print(\"=== ANALISI ESEMPIO CORRETTAMENTE CLASSIFICATO ===\")\n",
    "        \n",
    "        for images, true_labels in self.data_manager.testloader:\n",
    "            images, true_labels = images.to(self.device), true_labels.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(images)\n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Trova il primo esempio correttamente classificato nel batch\n",
    "            for i in range(len(images)):\n",
    "                if predicted_classes[i] == true_labels[i]:\n",
    "                    # Estrai la singola immagine e label\n",
    "                    image = images[i:i+1]  # Mantieni la dimensione batch\n",
    "                    true_label = true_labels[i]\n",
    "                    predicted_class = predicted_classes[i].item()\n",
    "                    \n",
    "                    print(f\"Esempio correttamente classificato:\")\n",
    "                    print(f\"Classe: {true_label.item()}\")\n",
    "                    \n",
    "                    # Analisi completa dell'esempio\n",
    "                    results = self.explainer.comprehensive_analysis(image, true_label.item())\n",
    "                    \n",
    "                    return image, true_label.item(), predicted_class, results\n",
    "        \n",
    "        return None, None, None, None\n",
    "    \n",
    "    def compare_explanation_methods(self, image: torch.Tensor, true_class: int):\n",
    "        \"\"\"Confronta i diversi metodi di explainability sullo stesso esempio\"\"\"\n",
    "        print(\"=== CONFRONTO METODI DI EXPLAINABILITY ===\")\n",
    "        \n",
    "        # GradCAM\n",
    "        gradcam_attr, pred_class = self.explainer.explain_with_gradcam(image)\n",
    "        \n",
    "        # Occlusion\n",
    "        occlusion_attr, _ = self.explainer.explain_with_occlusion(image)\n",
    "        \n",
    "        # Visualizzazione comparativa\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Prepara l'immagine\n",
    "        img = image.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Immagine originale (prima riga)\n",
    "        axes[0, 0].imshow(img)\n",
    "        axes[0, 0].set_title(f\"Originale\\\\nVero: {true_class}, Pred: {pred_class}\")\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # GradCAM\n",
    "        gradcam_viz = gradcam_attr.squeeze().detach().cpu().numpy()\n",
    "        if gradcam_viz.ndim == 3:\n",
    "            gradcam_viz = gradcam_viz.mean(axis=0)\n",
    "        gradcam_viz = (gradcam_viz - gradcam_viz.min()) / (gradcam_viz.max() - gradcam_viz.min() + 1e-8)\n",
    "        \n",
    "        axes[0, 1].imshow(gradcam_viz, cmap='hot')\n",
    "        axes[0, 1].set_title(\"GradCAM\")\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[0, 2].imshow(img, alpha=0.6)\n",
    "        axes[0, 2].imshow(gradcam_viz, cmap='hot', alpha=0.4)\n",
    "        axes[0, 2].set_title(\"GradCAM Overlay\")\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        # Occlusion\n",
    "        occlusion_viz = occlusion_attr.squeeze().detach().cpu().numpy()\n",
    "        if occlusion_viz.ndim == 3:\n",
    "            occlusion_viz = occlusion_viz.mean(axis=0)\n",
    "        occlusion_viz = (occlusion_viz - occlusion_viz.min()) / (occlusion_viz.max() - occlusion_viz.min() + 1e-8)\n",
    "        \n",
    "        axes[1, 0].axis('off')  # Vuoto per allineamento\n",
    "        \n",
    "        axes[1, 1].imshow(occlusion_viz, cmap='hot')\n",
    "        axes[1, 1].set_title(\"Occlusion\")\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(img, alpha=0.6)\n",
    "        axes[1, 2].imshow(occlusion_viz, cmap='hot', alpha=0.4)\n",
    "        axes[1, 2].set_title(\"Occlusion Overlay\")\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'gradcam': gradcam_attr,\n",
    "            'occlusion': occlusion_attr,\n",
    "            'predicted_class': pred_class\n",
    "        }\n",
    "\n",
    "# Inizializza il valutatore di trustworthiness\n",
    "trustworthiness_evaluator = TrustworthinessEvaluator(classifier, data_manager)\n",
    "\n",
    "print(\"Trustworthiness Evaluator inizializzato con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MNIST: 70.000 immagini di cifre scritte a mano (0-9), 28x28 pixel, 1 canale (convertito a 3 per DenseNet).\")\n",
    "\n",
    "# === VALUTAZIONE COMPLETA DEL MODELLO ===\n",
    "\n",
    "print(\"VALUTAZIONE TRUSTWORTHINESS DEL MODELLO MNIST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Valutazione su campioni casuali\n",
    "accuracy, avg_confidence = trustworthiness_evaluator.evaluate_sample_predictions(num_samples=20)\n",
    "\n",
    "print(f\"\\nRIEPILOGO PERFORMANCE:\")\n",
    "print(f\"   • Accuracy sui campioni: {accuracy:.1%}\")\n",
    "print(f\"   • Confidenza media: {avg_confidence:.3f}\")\n",
    "\n",
    "# 2. Analisi di un esempio correttamente classificato\n",
    "print(f\"\\nANALISI ESEMPIO CORRETTO:\")\n",
    "print(\"-\" * 40)\n",
    "correct_img, correct_true, correct_pred, correct_results = trustworthiness_evaluator.analyze_correct_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in testloader:\n",
    "    outputs = model(images)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    # Cerca il primo errore nel batch\n",
    "    for i in range(len(images)):\n",
    "        if preds[i] != labels[i]:\n",
    "            print(f\"Errore: Predetto {preds[i].item()}, Reale {labels[i].item()}\")\n",
    "            break\n",
    "    else:\n",
    "        continue  # Se non trova errori in questo batch, continua al prossimo\n",
    "    break  # Se trova un errore, esce dal loop esterno\n",
    "\n",
    "# 3. Analisi di un esempio mal classificato\n",
    "print(f\"\\nANALISI ESEMPIO MAL CLASSIFICATO:\")\n",
    "print(\"-\" * 40)\n",
    "error_img, error_true, error_pred, error_results = trustworthiness_evaluator.analyze_misclassified_example()\n",
    "\n",
    "if error_img is not None:\n",
    "    print(f\"\\nCONFRONTO TRA PREDIZIONE CORRETTA E ERRATA:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Confronta i metodi di explainability sull'esempio errato\n",
    "    comparison_results = trustworthiness_evaluator.compare_explanation_methods(error_img, error_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rule_based_classifier(img):\n",
    "    img = img.squeeze().numpy()\n",
    "    if img.sum() < 100:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "class StabilityAnalyzer:\n",
    "    \"\"\"Classe per analizzare la stabilità delle spiegazioni\"\"\"\n",
    "    \n",
    "    def __init__(self, explainer: ExplainabilityAnalyzer):\n",
    "        self.explainer = explainer\n",
    "        \n",
    "    def test_explanation_stability(self, image: torch.Tensor, num_runs: int = 5):\n",
    "        \"\"\"Testa la stabilità delle spiegazioni ripetendo l'analisi\"\"\"\n",
    "        print(f\"TEST STABILITÀ SPIEGAZIONI ({num_runs} runs)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        gradcam_correlations = []\n",
    "        occlusion_correlations = []\n",
    "        \n",
    "        # Esegui la prima spiegazione come riferimento\n",
    "        ref_gradcam, pred_class = self.explainer.explain_with_gradcam(image)\n",
    "        ref_occlusion, _ = self.explainer.explain_with_occlusion(image)\n",
    "        \n",
    "        # Appiattisci per calcolare correlazioni\n",
    "        ref_gradcam_flat = ref_gradcam.flatten().detach().cpu().numpy()\n",
    "        ref_occlusion_flat = ref_occlusion.flatten().detach().cpu().numpy()\n",
    "        \n",
    "        print(f\"Classe predetta: {pred_class}\")\n",
    "        \n",
    "        for run in range(1, num_runs):\n",
    "            print(f\"Run {run + 1}...\", end=\" \")\n",
    "            \n",
    "            # GradCAM\n",
    "            gradcam_attr, _ = self.explainer.explain_with_gradcam(image)\n",
    "            gradcam_flat = gradcam_attr.flatten().detach().cpu().numpy()\n",
    "            gradcam_corr = np.corrcoef(ref_gradcam_flat, gradcam_flat)[0, 1]\n",
    "            gradcam_correlations.append(gradcam_corr)\n",
    "            \n",
    "            # Occlusion\n",
    "            occlusion_attr, _ = self.explainer.explain_with_occlusion(image)\n",
    "            occlusion_flat = occlusion_attr.flatten().detach().cpu().numpy()\n",
    "            occlusion_corr = np.corrcoef(ref_occlusion_flat, occlusion_flat)[0, 1]\n",
    "            occlusion_correlations.append(occlusion_corr)\n",
    "            \n",
    "            print(f\"GradCAM: {gradcam_corr:.3f}, Occlusion: {occlusion_corr:.3f}\")\n",
    "        \n",
    "        # Statistiche\n",
    "        gradcam_mean = np.mean(gradcam_correlations)\n",
    "        gradcam_std = np.std(gradcam_correlations)\n",
    "        occlusion_mean = np.mean(occlusion_correlations)\n",
    "        occlusion_std = np.std(occlusion_correlations)\n",
    "        \n",
    "        print(f\"\\nRISULTATI STABILITÀ:\")\n",
    "        print(f\"GradCAM - Media: {gradcam_mean:.3f} ± {gradcam_std:.3f}\")\n",
    "        print(f\"Occlusion - Media: {occlusion_mean:.3f} ± {occlusion_std:.3f}\")\n",
    "        \n",
    "        if gradcam_mean > 0.9 and occlusion_mean > 0.9:\n",
    "            print(\"Spiegazioni STABILI (correlazione > 0.9)\")\n",
    "        elif gradcam_mean > 0.7 and occlusion_mean > 0.7:\n",
    "            print(\"Spiegazioni MODERATAMENTE stabili (correlazione > 0.7)\")\n",
    "        else:\n",
    "            print(\"Spiegazioni INSTABILI (correlazione < 0.7)\")\n",
    "        \n",
    "        return {\n",
    "            'gradcam_correlations': gradcam_correlations,\n",
    "            'occlusion_correlations': occlusion_correlations,\n",
    "            'gradcam_stats': (gradcam_mean, gradcam_std),\n",
    "            'occlusion_stats': (occlusion_mean, occlusion_std)\n",
    "        }\n",
    "\n",
    "# Test della stabilità delle spiegazioni\n",
    "if correct_img is not None:\n",
    "    print(f\"\\nANALISI STABILITÀ DELLE SPIEGAZIONI\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    stability_analyzer = StabilityAnalyzer(explainer)\n",
    "    stability_results = stability_analyzer.test_explanation_stability(correct_img, num_runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errore_img = None\n",
    "errore_label = None\n",
    "errore_pred = None\n",
    "\n",
    "for images, labels in testloader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # Cerca il primo errore nel batch\n",
    "        for i in range(len(images)):\n",
    "            if preds[i] != labels[i]:\n",
    "                print(f\"Errore: Predetto {preds[i].item()}, Reale {labels[i].item()}\")\n",
    "                errore_img = images[i:i+1]  # Mantieni dimensione batch\n",
    "                errore_label = labels[i].item()\n",
    "                errore_pred = preds[i].item()\n",
    "                break\n",
    "        \n",
    "        # Se trovato un errore, esci dal loop esterno\n",
    "        if errore_img is not None:\n",
    "            break\n",
    "\n",
    "class TrustworthinessMetrics:\n",
    "    \"\"\"Classe per calcolare metriche quantitative di trustworthiness\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "    def calculate_prediction_confidence_distribution(self, dataloader: DataLoader, max_samples: int = 1000):\n",
    "        \"\"\"Calcola la distribuzione delle confidenze delle predizioni\"\"\"\n",
    "        print(\"ANALISI DISTRIBUZIONE CONFIDENZE\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        confidences = []\n",
    "        correct_confidences = []\n",
    "        incorrect_confidences = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        samples_processed = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in dataloader:\n",
    "                if samples_processed >= max_samples:\n",
    "                    break\n",
    "                    \n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                \n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                max_probs = torch.max(probabilities, dim=1)[0]\n",
    "                \n",
    "                for i in range(len(images)):\n",
    "                    confidence = max_probs[i].item()\n",
    "                    confidences.append(confidence)\n",
    "                    \n",
    "                    if predicted_classes[i] == labels[i]:\n",
    "                        correct_confidences.append(confidence)\n",
    "                    else:\n",
    "                        incorrect_confidences.append(confidence)\n",
    "                \n",
    "                samples_processed += len(images)\n",
    "        \n",
    "        # Statistiche\n",
    "        print(f\"Campioni analizzati: {len(confidences)}\")\n",
    "        print(f\"Confidenza media: {np.mean(confidences):.3f}\")\n",
    "        print(f\"Confidenza mediana: {np.median(confidences):.3f}\")\n",
    "        print(f\"Deviazione standard: {np.std(confidences):.3f}\")\n",
    "        \n",
    "        if correct_confidences and incorrect_confidences:\n",
    "            print(f\"Confidenza predizioni corrette: {np.mean(correct_confidences):.3f}\")\n",
    "            print(f\"Confidenza predizioni errate: {np.mean(incorrect_confidences):.3f}\")\n",
    "            \n",
    "            # Visualizzazione\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(confidences, bins=30, alpha=0.7, color='blue', label='Tutte')\n",
    "            plt.hist(correct_confidences, bins=30, alpha=0.7, color='green', label='Corrette')\n",
    "            plt.hist(incorrect_confidences, bins=30, alpha=0.7, color='red', label='Errate')\n",
    "            plt.xlabel('Confidenza')\n",
    "            plt.ylabel('Frequenza')\n",
    "            plt.title('Distribuzione Confidenze')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.boxplot([correct_confidences, incorrect_confidences], \n",
    "                       labels=['Corrette', 'Errate'])\n",
    "            plt.ylabel('Confidenza')\n",
    "            plt.title('Boxplot Confidenze')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return {\n",
    "            'all_confidences': confidences,\n",
    "            'correct_confidences': correct_confidences,\n",
    "            'incorrect_confidences': incorrect_confidences,\n",
    "            'mean_confidence': np.mean(confidences),\n",
    "            'confidence_std': np.std(confidences)\n",
    "        }\n",
    "    \n",
    "    def evaluate_calibration(self, dataloader: DataLoader, max_samples: int = 1000, n_bins: int = 10):\n",
    "        \"\"\"Valuta la calibrazione del modello (reliability diagram)\"\"\"\n",
    "        print(\"VALUTAZIONE CALIBRAZIONE MODELLO\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        all_confidences = []\n",
    "        all_accuracies = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        samples_processed = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in dataloader:\n",
    "                if samples_processed >= max_samples:\n",
    "                    break\n",
    "                    \n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                \n",
    "                predicted_classes = torch.argmax(outputs, dim=1)\n",
    "                max_probs = torch.max(probabilities, dim=1)[0]\n",
    "                \n",
    "                confidences = max_probs.cpu().numpy()\n",
    "                accuracies = (predicted_classes == labels).float().cpu().numpy()\n",
    "                \n",
    "                all_confidences.extend(confidences)\n",
    "                all_accuracies.extend(accuracies)\n",
    "                \n",
    "                samples_processed += len(images)\n",
    "        \n",
    "        # Crea i bin per la calibrazione\n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_lowers = bin_boundaries[:-1]\n",
    "        bin_uppers = bin_boundaries[1:]\n",
    "        \n",
    "        bin_confidences = []\n",
    "        bin_accuracies = []\n",
    "        bin_counts = []\n",
    "        \n",
    "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "            in_bin = np.logical_and(np.array(all_confidences) > bin_lower, \n",
    "                                   np.array(all_confidences) <= bin_upper)\n",
    "            prop_in_bin = in_bin.mean()\n",
    "            \n",
    "            if prop_in_bin > 0:\n",
    "                accuracy_in_bin = np.array(all_accuracies)[in_bin].mean()\n",
    "                avg_confidence_in_bin = np.array(all_confidences)[in_bin].mean()\n",
    "                bin_accuracies.append(accuracy_in_bin)\n",
    "                bin_confidences.append(avg_confidence_in_bin)\n",
    "                bin_counts.append(in_bin.sum())\n",
    "            else:\n",
    "                bin_accuracies.append(0)\n",
    "                bin_confidences.append(0)\n",
    "                bin_counts.append(0)\n",
    "        \n",
    "        # Expected Calibration Error (ECE)\n",
    "        ece = 0\n",
    "        for i in range(n_bins):\n",
    "            if bin_counts[i] > 0:\n",
    "                ece += (bin_counts[i] / len(all_confidences)) * abs(bin_confidences[i] - bin_accuracies[i])\n",
    "        \n",
    "        print(f\"Expected Calibration Error (ECE): {ece:.4f}\")\n",
    "        \n",
    "        # Visualizzazione reliability diagram\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar(range(n_bins), bin_accuracies, alpha=0.7, label='Accuracy', width=0.8)\n",
    "        plt.plot(range(n_bins), bin_confidences, 'ro-', label='Confidence', linewidth=2)\n",
    "        plt.plot([0, n_bins-1], [bin_lowers[0], bin_uppers[-1]], 'k--', label='Perfect Calibration')\n",
    "        \n",
    "        plt.xlabel('Confidence Bin')\n",
    "        plt.ylabel('Accuracy / Confidence')\n",
    "        plt.title(f'Reliability Diagram (ECE: {ece:.4f})')\n",
    "        plt.legend()\n",
    "        plt.xticks(range(n_bins), [f'{bin_lowers[i]:.1f}-{bin_uppers[i]:.1f}' for i in range(n_bins)], rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'ece': ece,\n",
    "            'bin_accuracies': bin_accuracies,\n",
    "            'bin_confidences': bin_confidences,\n",
    "            'bin_counts': bin_counts\n",
    "        }\n",
    "\n",
    "# Valutazione quantitativa della trustworthiness\n",
    "print(\"\\nVALUTAZIONE QUANTITATIVA TRUSTWORTHINESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metrics_evaluator = TrustworthinessMetrics(model, device)\n",
    "\n",
    "# Analisi distribuzione confidenze\n",
    "confidence_analysis = metrics_evaluator.calculate_prediction_confidence_distribution(testloader, max_samples=1000)\n",
    "\n",
    "# Analisi calibrazione\n",
    "calibration_analysis = metrics_evaluator.evaluate_calibration(testloader, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from captum.attr import LayerGradCam, LayerAttribution, Occlusion\n",
    "\n",
    "# Usa il modello già addestrato invece di crearne uno nuovo\n",
    "model = classifier.get_model()\n",
    "model.eval()\n",
    "\n",
    "def show_saliency_on_image(input_tensor, attributions, title=\"Saliency Map\"):\n",
    "    \"\"\"Funzione per visualizzare la saliency map sull'immagine\"\"\"\n",
    "    \n",
    "    # Prepara l'immagine per la visualizzazione\n",
    "    img = input_tensor.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
    "    \n",
    "    # Denormalizza l'immagine (ImageNet normalization)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    # Prepara la saliency map\n",
    "    saliency = attributions.squeeze().detach().cpu().numpy()\n",
    "    if saliency.ndim == 3:\n",
    "        saliency = saliency.mean(axis=0)\n",
    "    \n",
    "    # Normalizza la saliency map\n",
    "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
    "    \n",
    "    # Crea la visualizzazione\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Immagine originale\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Immagine Originale\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Saliency map\n",
    "    im = axes[1].imshow(saliency, cmap='hot')\n",
    "    axes[1].set_title(\"Saliency Map\")\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(img, alpha=0.6)\n",
    "    axes[2].imshow(saliency, cmap='hot', alpha=0.4)\n",
    "    axes[2].set_title(title)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Verifica che errore_img esista\n",
    "if 'errore_img' in locals() and errore_img is not None:\n",
    "    target_layer = model.features[-1]\n",
    "    gradcam = LayerGradCam(model, target_layer)\n",
    "    occlusion = Occlusion(model)\n",
    "\n",
    "    attributions = gradcam.attribute(errore_img, target=errore_label)\n",
    "    upsampled_attr = LayerAttribution.interpolate(attributions, errore_img.shape[2:])\n",
    "    show_saliency_on_image(errore_img, upsampled_attr, title=f\"Grad-CAM (Predetto {errore_pred}, Reale {errore_label})\")\n",
    "\n",
    "    attr_occ = occlusion.attribute(\n",
    "        errore_img,\n",
    "        strides=(3, 8, 8),\n",
    "        target=errore_label,\n",
    "        sliding_window_shapes=(3, 15, 15)\n",
    "    )\n",
    "    show_saliency_on_image(errore_img, attr_occ, title=f\"Occlusion (Predetto {errore_pred}, Reale {errore_label})\")\n",
    "else:\n",
    "    print(\"Nessun esempio di errore trovato per l'analisi saliency.\")\n",
    "\n",
    "# === CONCLUSIONI E RIEPILOGO ===\n",
    "\n",
    "print(\"\\nRIEPILOGO ANALISI TRUSTWORTHINESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"RISULTATI PRINCIPALI:\")\n",
    "print(f\"   1. Accuracy dopo fine-tuning: {accuracy_after:.2f}%\")\n",
    "print(f\"   2. Expected Calibration Error: {calibration_analysis['ece']:.4f}\")\n",
    "print(f\"   3. Confidenza media: {confidence_analysis['mean_confidence']:.3f}\")\n",
    "\n",
    "print(f\"\\nMETODOLOGIE IMPLEMENTATE:\")\n",
    "print(f\"   • GradCAM: Evidenziazione regioni importanti tramite gradienti\")\n",
    "print(f\"   • Occlusion: Analisi importanza tramite occlusione sistematica\")\n",
    "if LIME_AVAILABLE:\n",
    "    print(f\"   • LIME: Spiegazioni locali tramite perturbazioni\")\n",
    "print(f\"   • Analisi stabilità: Consistenza delle spiegazioni\")\n",
    "print(f\"   • Calibrazione: Affidabilità delle confidenze\")\n",
    "\n",
    "print(f\"\\nARCHITETTURA MODULARE:\")\n",
    "print(f\"   • DataManager: Gestione dati e preprocessing\")\n",
    "print(f\"   • MNISTClassifier: Fine-tuning e valutazione modello\") \n",
    "print(f\"   • ExplainabilityAnalyzer: Analisi XAI integrate\")\n",
    "print(f\"   • TrustworthinessEvaluator: Valutazione complessiva\")\n",
    "print(f\"   • StabilityAnalyzer: Test robustezza spiegazioni\")\n",
    "print(f\"   • TrustworthinessMetrics: Metriche quantitative\")\n",
    "\n",
    "print(f\"\\nPROBLEMI RISOLTI:\")\n",
    "print(f\"   • Ultimo layer non addestrato: RISOLTO con fine-tuning\")\n",
    "print(f\"   • Accuracy ~0.08%: MIGLIORATO a {accuracy_after:.2f}%\")\n",
    "print(f\"   • Struttura non modulare: RISOLTO con architettura a classi\")\n",
    "print(f\"   • Mancanza di valutazione quantitativa: AGGIUNTO analisi metriche\")\n",
    "\n",
    "print(f\"\\nTRUSTWORTHINESS ASSESSMENT:\")\n",
    "if accuracy_after > 80:\n",
    "    print(\"   ALTA: Modello affidabile per analisi XAI\")\n",
    "elif accuracy_after > 60:\n",
    "    print(\"   MEDIA: Modello utilizzabile con cautela\")\n",
    "else:\n",
    "    print(\"   BASSA: Modello necessita ulteriore training\")\n",
    "\n",
    "if calibration_analysis['ece'] < 0.1:\n",
    "    print(\"   Ben calibrato: Confidenze affidabili\")\n",
    "else:\n",
    "    print(\"   Mal calibrato: Confidenze da interpretare con cautela\")\n",
    "\n",
    "print(f\"\\nINSIGHTS FINALI:\")\n",
    "print(f\"   • Il fine-tuning dell'ultimo layer è ESSENZIALE per XAI significativa\")\n",
    "print(f\"   • L'architettura modulare facilita estensioni e manutenzione\")\n",
    "print(f\"   • La valutazione quantitativa è cruciale per la trustworthiness\")\n",
    "print(f\"   • GradCAM e Occlusion forniscono spiegazioni complementari\")\n",
    "\n",
    "print(f\"\\nPOSSIBILI ESTENSIONI:\")\n",
    "print(f\"   • Implementazione di altri metodi XAI (SHAP, Integrated Gradients)\")\n",
    "print(f\"   • Analisi su dataset più complessi\")\n",
    "print(f\"   • Studio dell'interpretabilità cross-domain\")\n",
    "print(f\"   • Sviluppo di metriche custom di trustworthiness\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALISI COMPLETATA CON SUCCESSO!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
