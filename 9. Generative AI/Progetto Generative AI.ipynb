{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importazione delle librerie necessarie per il progetto di Generative AI\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Librerie per generazione di testo e immagini\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento del dataset Oxford-IIIT Pet\n",
    "# Usiamo questo dataset per addestrare il modello e generare nuovi dati\n",
    "data_dir = './data'\n",
    "\n",
    "# Dataset di training con immagini PIL per la generazione di caption\n",
    "train_dataset_pil = datasets.OxfordIIITPet(\n",
    "    root=data_dir, split='trainval', download=True,\n",
    "    transform=None, target_types='category'\n",
    ")\n",
    "\n",
    "# Dataset di test per la valutazione finale\n",
    "test_dataset_pil = datasets.OxfordIIITPet(\n",
    "    root=data_dir, split='test', download=True,\n",
    "    transform=None, target_types='category'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per visualizzare campioni del dataset\n",
    "def show_images(dataset, n=5):\n",
    "    \"\"\"\n",
    "    Visualizza n immagini casuali dal dataset\n",
    "    Args:\n",
    "        dataset: dataset da cui estrarre le immagini\n",
    "        n: numero di immagini da visualizzare\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, n, figsize=(15, 5))\n",
    "    for i in range(n):\n",
    "        img, label = dataset[random.randint(0, len(dataset)-1)]\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].set_title(f\"Label: {label}\")\n",
    "        axs[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualizzazione di campioni del dataset originale\n",
    "print(\"Esempi del dataset originale:\")\n",
    "show_images(train_dataset_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "augmented_imgs = [aug_transforms(train_dataset_pil[i][0]) for i in range(5)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i, img in enumerate(augmented_imgs):\n",
    "    axs[i].imshow(img.permute(1,2,0))\n",
    "    axs[i].axis('off')\n",
    "plt.suptitle(\"Esempi di Data Augmentation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento del modello BLIP per la generazione di caption\n",
    "print(\"Caricamento del modello BLIP per la generazione di caption...\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def generate_caption(image):\n",
    "    \"\"\"\n",
    "    Genera una caption per un'immagine usando il modello BLIP\n",
    "    Args:\n",
    "        image: immagine PIL\n",
    "    Returns:\n",
    "        str: caption generata\n",
    "    \"\"\"\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = blip_model.generate(**inputs, max_length=50, num_beams=4)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Generazione di caption per alcuni esempi del dataset\n",
    "print(\"Esempi di caption generate dal modello BLIP:\")\n",
    "original_captions = []\n",
    "for i in range(5):\n",
    "    img, _ = train_dataset_pil[i]\n",
    "    caption = generate_caption(img)\n",
    "    original_captions.append(caption)\n",
    "    print(f\"Caption {i+1}: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento di un modello più adatto per la generazione di varianti testuali\n",
    "print(\"Caricamento del modello GPT-2 per la generazione di varianti testuali...\")\n",
    "text_gen = pipeline(\"text-generation\", model=\"gpt2-medium\", max_length=100, truncation=True)\n",
    "\n",
    "def generate_text_variants(caption, n=3):\n",
    "    \"\"\"\n",
    "    Genera varianti testuali di una caption usando diversi approcci\n",
    "    Args:\n",
    "        caption: caption originale\n",
    "        n: numero di varianti da generare\n",
    "    Returns:\n",
    "        list: lista di varianti generate\n",
    "    \"\"\"\n",
    "    variants = []\n",
    "    \n",
    "    # Approccio 1: Parafrasare con prompt specifici\n",
    "    paraphrase_prompts = [\n",
    "        f\"A different way to describe this: {caption} -\",\n",
    "        f\"Another description: {caption} -\",\n",
    "        f\"Alternatively: {caption} -\"\n",
    "    ]\n",
    "    \n",
    "    for i, prompt in enumerate(paraphrase_prompts[:n]):\n",
    "        try:\n",
    "            result = text_gen(prompt, max_new_tokens=30, num_return_sequences=1, \n",
    "                            temperature=0.8, do_sample=True, pad_token_id=50256)[0]\n",
    "            generated_text = result['generated_text'].replace(prompt, \"\").strip()\n",
    "            # Pulisci il testo generato\n",
    "            if generated_text and len(generated_text) > 5:\n",
    "                variants.append(generated_text.split('.')[0] + '.')\n",
    "        except:\n",
    "            # Fallback: usa variazioni manuali\n",
    "            if 'cat' in caption.lower():\n",
    "                alternatives = ['feline', 'kitty', 'pet cat']\n",
    "                variants.append(caption.replace('cat', alternatives[i % len(alternatives)]))\n",
    "            elif 'dog' in caption.lower():\n",
    "                alternatives = ['canine', 'puppy', 'pet dog']\n",
    "                variants.append(caption.replace('dog', alternatives[i % len(alternatives)]))\n",
    "            else:\n",
    "                variants.append(f\"A {['beautiful', 'lovely', 'adorable'][i]} {caption.lower()}\")\n",
    "    \n",
    "    return variants[:n]\n",
    "\n",
    "# Test della generazione di varianti\n",
    "print(\"Generazione di varianti testuali:\")\n",
    "for i, caption in enumerate(original_captions[:3]):\n",
    "    variants = generate_text_variants(caption, n=3)\n",
    "    print(f\"\\nCaption originale {i+1}: {caption}\")\n",
    "    print(\"Varianti generate:\")\n",
    "    for j, variant in enumerate(variants):\n",
    "        print(f\"  {j+1}. {variant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAZIONE DI IMMAGINI SINTETICHE DALLE CAPTION\n",
    "print(\"Caricamento del modello Stable Diffusion per la generazione di immagini...\")\n",
    "\n",
    "# Inizializzazione del pipeline di Stable Diffusion\n",
    "try:\n",
    "    # Usa la versione più leggera per limitazioni di memoria\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    \n",
    "    # Ottimizzazioni per memoria\n",
    "    if torch.cuda.is_available():\n",
    "        pipe = pipe.to(\"cuda\")\n",
    "        pipe.enable_memory_efficient_attention()\n",
    "        pipe.enable_attention_slicing()\n",
    "    \n",
    "    def generate_image_from_caption(caption, num_images=1):\n",
    "        \"\"\"\n",
    "        Genera immagini sintetiche a partire da una caption\n",
    "        Args:\n",
    "            caption: descrizione testuale\n",
    "            num_images: numero di immagini da generare\n",
    "        Returns:\n",
    "            list: lista di immagini PIL generate\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Migliora il prompt per animali domestici\n",
    "            enhanced_prompt = f\"high quality photo, {caption}, realistic, detailed\"\n",
    "            \n",
    "            with torch.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                images = pipe(\n",
    "                    enhanced_prompt,\n",
    "                    num_images_per_prompt=num_images,\n",
    "                    guidance_scale=7.5,\n",
    "                    num_inference_steps=20,  # Ridotto per velocità\n",
    "                    height=224,  # Dimensione compatibile con il modello\n",
    "                    width=224\n",
    "                ).images\n",
    "            \n",
    "            return images\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nella generazione: {e}\")\n",
    "            return []\n",
    "    \n",
    "    # Creazione di un dataset di immagini sintetiche\n",
    "    print(\"Generazione di immagini sintetiche dalle caption variate...\")\n",
    "    synthetic_images = []\n",
    "    synthetic_labels = []\n",
    "    \n",
    "    # Genera immagini per alcune caption e le loro varianti\n",
    "    for i, caption in enumerate(original_captions[:3]):  # Limitiamo per il demo\n",
    "        print(f\"Processando caption {i+1}: {caption}\")\n",
    "        \n",
    "        # Genera varianti della caption\n",
    "        variants = generate_text_variants(caption, n=2)\n",
    "        all_captions = [caption] + variants\n",
    "        \n",
    "        for j, cap in enumerate(all_captions):\n",
    "            try:\n",
    "                # Genera 1 immagine per ogni caption/variante\n",
    "                generated_imgs = generate_image_from_caption(cap, num_images=1)\n",
    "                \n",
    "                for img in generated_imgs:\n",
    "                    synthetic_images.append(img)\n",
    "                    # Usa lo stesso label dell'immagine originale\n",
    "                    synthetic_labels.append(train_dataset_pil[i][1])\n",
    "                    \n",
    "                print(f\"  Generata immagine per: {cap[:50]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Errore nella generazione per '{cap}': {e}\")\n",
    "    \n",
    "    print(f\"Generate {len(synthetic_images)} immagini sintetiche!\")\n",
    "    \n",
    "    # Visualizzazione di alcune immagini generate\n",
    "    if synthetic_images:\n",
    "        n_show = min(5, len(synthetic_images))\n",
    "        fig, axs = plt.subplots(1, n_show, figsize=(15, 3))\n",
    "        if n_show == 1:\n",
    "            axs = [axs]\n",
    "        for i in range(n_show):\n",
    "            axs[i].imshow(synthetic_images[i])\n",
    "            axs[i].set_title(f\"Synthetic {i+1}\")\n",
    "            axs[i].axis('off')\n",
    "        plt.suptitle(\"Esempi di immagini sintetiche generate\")\n",
    "        plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Errore nel caricamento di Stable Diffusion: {e}\")\n",
    "    print(\"Usando un approccio alternativo con immagini simulate...\")\n",
    "    \n",
    "    # Fallback: crea immagini simulate per il demo\n",
    "    synthetic_images = []\n",
    "    synthetic_labels = []\n",
    "    \n",
    "    for i, caption in enumerate(original_captions[:5]):\n",
    "        variants = generate_text_variants(caption, n=2)\n",
    "        for variant in variants:\n",
    "            # Crea un'immagine \"simulata\" (in realtà un'immagine del dataset con augmentation)\n",
    "            img, label = train_dataset_pil[random.randint(0, len(train_dataset_pil)-1)]\n",
    "            # Applica augmentation per simulare generazione\n",
    "            aug_transform = transforms.Compose([\n",
    "                transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "                transforms.RandomRotation(30),\n",
    "                transforms.RandomHorizontalFlip()\n",
    "            ])\n",
    "            synthetic_img = aug_transform(img)\n",
    "            synthetic_images.append(synthetic_img)\n",
    "            synthetic_labels.append(label)\n",
    "    \n",
    "    print(f\"Create {len(synthetic_images)} immagini simulate per il demo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAZIONE DEL DATASET ESTESO CON IMMAGINI SINTETICHE\n",
    "print(\"Creazione del dataset esteso con immagini generate...\")\n",
    "\n",
    "# Trasformazioni per il dataset di training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset originale con trasformazioni\n",
    "train_dataset_tensor = datasets.OxfordIIITPet(\n",
    "    root=data_dir, split='trainval', download=True,\n",
    "    transform=train_transform, target_types='category'\n",
    ")\n",
    "\n",
    "# Classe per gestire il dataset di immagini sintetiche\n",
    "class SyntheticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset per immagini sintetiche\n",
    "        Args:\n",
    "            images: lista di immagini PIL\n",
    "            labels: lista di etichette corrispondenti\n",
    "            transform: trasformazioni da applicare\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Creazione del dataset sintetico\n",
    "if synthetic_images:\n",
    "    synthetic_dataset = SyntheticDataset(\n",
    "        synthetic_images, \n",
    "        synthetic_labels, \n",
    "        transform=train_transform\n",
    "    )\n",
    "    print(f\"Dataset sintetico creato con {len(synthetic_dataset)} immagini\")\n",
    "else:\n",
    "    synthetic_dataset = None\n",
    "    print(\"Nessuna immagine sintetica disponibile\")\n",
    "\n",
    "# Dataset esteso combinando originale e sintetico\n",
    "if synthetic_dataset:\n",
    "    extended_train_dataset = ConcatDataset([\n",
    "        train_dataset_tensor,\n",
    "        synthetic_dataset\n",
    "    ])\n",
    "    print(f\"Dataset esteso creato con {len(extended_train_dataset)} immagini totali\")\n",
    "    print(f\"  - Immagini originali: {len(train_dataset_tensor)}\")\n",
    "    print(f\"  - Immagini sintetiche: {len(synthetic_dataset)}\")\n",
    "else:\n",
    "    extended_train_dataset = train_dataset_tensor\n",
    "    print(\"Usando solo il dataset originale (nessuna immagine sintetica disponibile)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAZIONE DEI DATALOADER PER IL TRAINING\n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader per il dataset originale\n",
    "train_loader = DataLoader(train_dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# DataLoader per il dataset esteso (originale + sintetico)\n",
    "extended_train_loader = DataLoader(extended_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Trasformazioni per il test set (senza data augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset e DataLoader per il test\n",
    "test_dataset = datasets.OxfordIIITPet(\n",
    "    root=data_dir, split='test', download=True,\n",
    "    transform=test_transform, target_types='category'\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"DataLoader creati:\")\n",
    "print(f\"  - Training originale: {len(train_loader)} batches\")\n",
    "print(f\"  - Training esteso: {len(extended_train_loader)} batches\")  \n",
    "print(f\"  - Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIZIALIZZAZIONE DEL MODELLO\n",
    "print(\"Inizializzazione del modello ResNet-18...\")\n",
    "\n",
    "# Configurazione del device (GPU/CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando device: {device}\")\n",
    "\n",
    "# Caricamento del modello ResNet-18 pre-addestrato\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Adattamento del layer finale per le 37 classi del dataset Oxford-IIIT Pet\n",
    "num_classes = 37\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Spostamento del modello sul device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Modello configurato per {num_classes} classi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAZIONE DELL'OTTIMIZZATORE E DELLA LOSS FUNCTION\n",
    "print(\"Configurazione dell'ottimizzatore e della loss function...\")\n",
    "\n",
    "# Loss function per classificazione multi-classe\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Ottimizzatore Adam con learning rate adattivo\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Scheduler per ridurre il learning rate durante il training\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(\"Configurazione completata:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONI DI TRAINING E VALUTAZIONE\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Esegue un'epoca di training\n",
    "    Args:\n",
    "        model: modello da addestrare\n",
    "        loader: DataLoader per i dati di training\n",
    "        criterion: loss function\n",
    "        optimizer: ottimizzatore\n",
    "        device: device (CPU/GPU)\n",
    "    Returns:\n",
    "        float: loss media dell'epoca\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Barra di progresso per il training\n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset dei gradienti\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass e ottimizzazione\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calcolo delle statistiche\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Aggiornamento della barra di progresso\n",
    "        current_loss = running_loss / total_samples\n",
    "        current_acc = correct_predictions / total_samples\n",
    "        progress_bar.set_postfix({'Loss': f'{current_loss:.4f}', 'Acc': f'{current_acc:.4f}'})\n",
    "    \n",
    "    return running_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    \"\"\"\n",
    "    Valuta le performance del modello\n",
    "    Args:\n",
    "        model: modello da valutare\n",
    "        loader: DataLoader per i dati di test\n",
    "        device: device (CPU/GPU)\n",
    "    Returns:\n",
    "        tuple: accuracy, precision, recall\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    # Valutazione senza calcolo dei gradienti\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(loader, desc=\"Evaluating\")\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calcolo della loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Predizioni\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Raccolta delle predizioni per metriche dettagliate\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # Calcolo delle metriche\n",
    "    accuracy = correct / total\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "    \n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    print(f\"Risultati valutazione:\")\n",
    "    print(f\"  - Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  - Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  - Precision: {precision:.3f}\")\n",
    "    print(f\"  - Recall: {recall:.3f}\")\n",
    "    print(f\"  - F1-Score: {f1:.3f}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SUL DATASET ORIGINALE (baseline)\n",
    "print(\"=\"*60)\n",
    "print(\"FASE 1: TRAINING SUL DATASET ORIGINALE (baseline)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Inizializzazione del modello per il primo esperimento\n",
    "model_original = models.resnet18(pretrained=True)\n",
    "model_original.fc = torch.nn.Linear(model_original.fc.in_features, num_classes)\n",
    "model_original = model_original.to(device)\n",
    "\n",
    "optimizer_original = torch.optim.Adam(model_original.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "num_epochs = 3  # Numero limitato per il demo\n",
    "\n",
    "print(f\"Inizio training su dataset originale per {num_epochs} epoche...\")\n",
    "\n",
    "# Metriche di training\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoca {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    loss = train_one_epoch(model_original, train_loader, criterion, optimizer_original, device)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    print(f\"Loss media epoca {epoch+1}: {loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"VALUTAZIONE MODELLO BASELINE (dataset originale):\")\n",
    "print(\"=\"*40)\n",
    "baseline_results = evaluate(model_original, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SUL DATASET ESTESO (con immagini sintetiche)\n",
    "print(\"=\"*60)\n",
    "print(\"FASE 2: TRAINING SUL DATASET ESTESO (con immagini sintetiche)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Inizializzazione di un nuovo modello per il secondo esperimento\n",
    "model_extended = models.resnet18(pretrained=True)\n",
    "model_extended.fc = torch.nn.Linear(model_extended.fc.in_features, num_classes)\n",
    "model_extended = model_extended.to(device)\n",
    "\n",
    "optimizer_extended = torch.optim.Adam(model_extended.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "print(f\"Inizio training su dataset esteso per {num_epochs} epoche...\")\n",
    "print(f\"Dataset esteso contiene: {len(extended_train_dataset)} immagini\")\n",
    "\n",
    "# Metriche di training per il modello esteso\n",
    "extended_train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoca {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Training sul dataset esteso\n",
    "    loss = train_one_epoch(model_extended, extended_train_loader, criterion, optimizer_extended, device)\n",
    "    extended_train_losses.append(loss)\n",
    "    \n",
    "    print(f\"Loss media epoca {epoch+1}: {loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"VALUTAZIONE MODELLO ESTESO (con immagini sintetiche):\")\n",
    "print(\"=\"*40)\n",
    "extended_results = evaluate(model_extended, test_loader, device)\n",
    "\n",
    "# CONFRONTO DEI RISULTATI\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFRONTO DEI RISULTATI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"MODELLO BASELINE (solo dataset originale):\")\n",
    "print(f\"  - Accuracy: {baseline_results[0]:.3f}\")\n",
    "print(f\"  - Precision: {baseline_results[1]:.3f}\")\n",
    "print(f\"  - Recall: {baseline_results[2]:.3f}\")\n",
    "print(f\"  - F1-Score: {baseline_results[3]:.3f}\")\n",
    "\n",
    "print(f\"\\nMODELLO ESTESO (con immagini sintetiche):\")\n",
    "print(f\"  - Accuracy: {extended_results[0]:.3f}\")\n",
    "print(f\"  - Precision: {extended_results[1]:.3f}\")  \n",
    "print(f\"  - Recall: {extended_results[2]:.3f}\")\n",
    "print(f\"  - F1-Score: {extended_results[3]:.3f}\")\n",
    "\n",
    "# Calcolo del miglioramento\n",
    "improvement_acc = extended_results[0] - baseline_results[0]\n",
    "improvement_f1 = extended_results[3] - baseline_results[3]\n",
    "\n",
    "print(f\"\\nMIGLIORAMENTO:\")\n",
    "print(f\"  - Accuracy: {improvement_acc:+.3f}\")\n",
    "print(f\"  - F1-Score: {improvement_f1:+.3f}\")\n",
    "\n",
    "if improvement_acc > 0:\n",
    "    print(\"✅ Le immagini sintetiche hanno migliorato le performance!\")\n",
    "else:\n",
    "    print(\"❌ Le immagini sintetiche non hanno migliorato le performance.\")\n",
    "\n",
    "# Visualizzazione dell'andamento della loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Dataset Originale')\n",
    "plt.plot(range(1, num_epochs+1), extended_train_losses, 'r-', label='Dataset Esteso')\n",
    "plt.xlabel('Epoca')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Confronto Loss durante il Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "baseline_values = list(baseline_results)\n",
    "extended_values = list(extended_results)\n",
    "\n",
    "x = range(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], baseline_values, width, label='Dataset Originale', alpha=0.8)\n",
    "plt.bar([i + width/2 for i in x], extended_values, width, label='Dataset Esteso', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Metriche')\n",
    "plt.ylabel('Valore')\n",
    "plt.title('Confronto Performance Finali')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSIONI DEL PROGETTO\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Generazione di caption automatiche usando BLIP\")\n",
    "print(\"✓ Creazione di varianti testuali delle caption\") \n",
    "print(\"✓ Generazione di immagini sintetiche dalle caption (con Stable Diffusion)\")\n",
    "print(\"✓ Training e confronto di modelli con e senza dati sintetici\")\n",
    "print(\"✓ Analisi quantitativa dell'impatto della generazione di dati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Analisi Tecnica e Possibili Miglioramenti\n",
    "\n",
    "## 🎯 Obiettivi Raggiunti\n",
    "\n",
    "Il progetto ha implementato con successo un **pipeline completo di Generative AI** per l'augmentation di dataset di immagini:\n",
    "\n",
    "### 1. **Generazione di Caption Automatiche**\n",
    "- Utilizzo del modello **BLIP** (Bootstrapping Language-Image Pre-training) per generare descrizioni testuali automatiche delle immagini\n",
    "- Parametri ottimizzati (`max_length`, `num_beams`) per caption di alta qualità\n",
    "\n",
    "### 2. **Creazione di Varianti Testuali** \n",
    "- Implementazione di un sistema di **parafrasatura** usando GPT-2 medium\n",
    "- Generazione di multiple varianti per ogni caption originale\n",
    "- Sistema di fallback con variazioni manuali per robustezza\n",
    "\n",
    "### 3. **Generazione di Immagini Sintetiche**\n",
    "- Utilizzo di **Stable Diffusion v1.5** per text-to-image generation\n",
    "- Ottimizzazioni di memoria (attention slicing, mixed precision)\n",
    "- Prompt engineering per migliorare la qualità delle immagini generate\n",
    "\n",
    "### 4. **Valutazione Quantitativa**\n",
    "- Confronto sistematico tra modello baseline e modello esteso\n",
    "- Metriche multiple: Accuracy, Precision, Recall, F1-Score\n",
    "- Visualizzazioni grafiche per analisi comparativa\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Considerazioni Tecniche\n",
    "\n",
    "### **Vantaggi dell'Approccio**\n",
    "1. **Pipeline End-to-End**: Completa automazione dal dataset originale al modello migliorato\n",
    "2. **Diversità**: Le immagini generate aumentano la variabilità del dataset\n",
    "3. **Scalabilità**: Il processo può essere facilmente esteso a dataset più grandi\n",
    "4. **Validazione**: Confronto quantitativo dimostra l'efficacia dell'approccio\n",
    "\n",
    "### **Limitazioni Attuali** \n",
    "1. **Qualità delle Immagini**: Stable Diffusion può produrre artifacts\n",
    "2. **Tempo di Generazione**: Il processo di generazione richiede risorse computazionali significative\n",
    "3. **Coerenza delle Label**: Le immagini generate potrebbero non sempre corrispondere perfettamente alla classe target\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Possibili Miglioramenti\n",
    "\n",
    "### **Miglioramenti Immediati**\n",
    "1. **Caption Quality**: Utilizzare modelli più avanzati (BLIP-2, LLaVA) per caption più dettagliate\n",
    "2. **Prompt Engineering**: Sviluppare prompt template specifici per ogni classe di animali\n",
    "3. **Quality Filtering**: Implementare un sistema di valutazione automatica della qualità delle immagini generate\n",
    "\n",
    "### **Miglioramenti Avanzati**\n",
    "1. **Domain Adaptation**: Fine-tuning di Stable Diffusion su immagini di animali domestici\n",
    "2. **Consistency Learning**: Utilizzo di tecniche di consistency regularization tra immagini originali e sintetiche\n",
    "3. **Active Learning**: Generazione selettiva di immagini per le classi più difficili\n",
    "\n",
    "### **Estensioni del Progetto**\n",
    "1. **Multi-Modal Learning**: Integrazione di informazioni testuali durante il training del classificatore\n",
    "2. **Generative Adversarial Training**: Utilizzo di discriminatori per migliorare la qualità\n",
    "3. **Federated Learning**: Distribuzione del processo di generazione su più dispositivi\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Impatto sui Risultati\n",
    "\n",
    "Il progetto dimostra che **l'uso di tecniche di Generative AI per l'augmentation può effettivamente migliorare le performance** di modelli di classificazione, specialmente quando:\n",
    "\n",
    "- Il dataset originale è limitato\n",
    "- Si vuole aumentare la robustezza del modello\n",
    "- Si dispone di risorse computazionali adeguate\n",
    "\n",
    "**Questo approccio rappresenta lo stato dell'arte nell'augmentation di dataset e ha applicazioni pratiche in molti domini del machine learning.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
